{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas started out in the financial world, so naturally it has strong timeseries support.\n",
    "\n",
    "The first half of this post will look at pandas' capabilities for manipulating time series data.\n",
    "The second half will discuss modelling time series data with statsmodels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style='ticks', context='talk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = web.DataReader(\"GS\", data_source='yahoo', start='2006-01-01',\n",
    "                    end='2010-01-01')\n",
    "gs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll call any `DataFrame` or `Series` with a `DatetimeIndex` a timeseries.\n",
    "There isn't a special data-container just for timeseries.\n",
    "That said, `DataFrames` and `Series` with a `DatetiemIndex` do gain some special behaviors and additional methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the elements of `gs.index`, we see that `DatetimeIndex`es are made up of `pandas.Timestamp`s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.index[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Timestamp` is mostly compatible with the `datetime.datetime` class, but much amenable to storage in arrays.\n",
    "\n",
    "Working with `Timestamp`s can be a be awkward, so Series and DataFrames with `DatetimeIndexes` have some special slicing rules.\n",
    "The first special case is *partial-string indexing*. Say we wanted to select all the days in 2006. Even with `Timestamp`'s convienient constructors, it's a pain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.loc[pd.Timestamp('2006-01-01'):pd.Timestamp('2006-12-31')].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to partial-string indexing, it's as simple as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.loc['2006'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since label slicing is inclusive, this slice selects any observation where the year is 2006."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second \"conveninece\" is `__getitem__` (squre-braket slicing) fallback indexing. I'm only going to mention it here, with the caveat that you should never use it.\n",
    "DataFrame `__getitem__` typically looks in the column: `gs['2006']` would search `gs.columns` for `'2006'`, not find it, and raise a `KeyError`. But DataFrames with a `DatetimeIndex` catch that `KeyError` and try to slice the index.\n",
    "If it succeeds in slicing the index, the result like `gs.loc['2006']` is returned.\n",
    "If it fails, the `KeyError` is reraised.\n",
    "This is confusing because in every other case `DataFrame.__getitem__` works on columns, and it's fragile becuase if you happened to have a column `'2006'` you *would* get just that column, and no fallback indexing would occur. Just use `gs.loc['2006']` when slicing DataFrame indexes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Resampling is similar to a `groupby`: you split the time series into groups (5-day buckets below), apply a function to each group (`mean`), and combine the result (one row per group)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.resample(\"5d\").mean().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.resample(\"W\").agg(['mean', 'sum']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can upsample to convert to a higher frequency.\n",
    "The new points are filled with NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.resample(\"6H\").mean().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling / Expanding / EW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These methods aren't unique to DatetimeIndexes, but they often make sense with timeseries, so I'll show them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.Close.plot(label='Raw')\n",
    "gs.Close.rolling(28).mean().plot(label='28D MA')\n",
    "gs.Close.expanding(7).mean().plot(label='7D EA')\n",
    "gs.Close.ewm(alpha=0.03).mean().plot(label='EWMA($\\\\alpha=.03$)')\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.25, .5))\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "plt.savefig('../output/images/ts-reew.svg', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of `.rolling`, `.expanding`, and `.ewm` return a deferred object, similar to a GroupBy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roll = gs.Close.rolling(30, center=True)\n",
    "roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = roll.agg(['mean', 'std'])\n",
    "ax = m['mean'].plot()\n",
    "ax.fill_between(m.index, m['mean'] - m['std'], m['mean'] + m['std'],\n",
    "                alpha=.25)\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "plt.savefig('../output/images/ts-roll.svg', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab Bag\n",
    "\n",
    "### Offsets\n",
    "\n",
    "These are similar to `dateutil.relativedelta`, but works with arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.index + pd.DateOffset(months=3, days=-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holiday Calendars\n",
    "\n",
    "There are a whole bunch of special calendars, useful for traders probabaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas.tseries.holiday import USColumbusDay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USColumbusDay.dates('2015-01-01', '2020-01-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timezones\n",
    "\n",
    "Pandas works with `pytz` for nice timezone-aware datetimes.\n",
    "The typical workflow is\n",
    "\n",
    "1. localize timezone-naive timestamps to some timezone\n",
    "2. convert to desired timezone\n",
    "\n",
    "If you already have timezone-aware Timestamps, there's no need for step one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tz naiive -> tz aware..... to desired UTC\n",
    "gs.tz_localize('US/Eastern').tz_convert('UTC').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Time Series\n",
    "\n",
    "The rest of this post will focus on time series in the econometric sense.\n",
    "My indented reader for this section isn't all that clear, so I apologize upfront for any sudden shifts in complexity.\n",
    "I'm roughly targeting material that could be presented in a first or second semester applied statisctics course.\n",
    "What follows certainly isn't a replacement for that.\n",
    "Any formality will be restricted to footnotes for the curious.\n",
    "I've put a whole bunch of resources at the end for people earger to learn more.\n",
    "\n",
    "We'll focus on modelling Average Monthly Flights. Let's download the data.\n",
    "If you've been following along in the series, you've seen most of this code before, so feel free to skip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import glob\n",
    "import zipfile\n",
    "\n",
    "import requests\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "def download_one(date):\n",
    "    '''\n",
    "    Download a single month's flights\n",
    "    '''\n",
    "    month = date.month\n",
    "    year = date.year\n",
    "    month_name = date.strftime('%B')\n",
    "    headers = {\n",
    "        'Pragma': 'no-cache',\n",
    "        'Origin': 'http://www.transtats.bts.gov',\n",
    "        'Accept-Encoding': 'gzip, deflate',\n",
    "        'Accept-Language': 'en-US,en;q=0.8',\n",
    "        'Upgrade-Insecure-Requests': '1',\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.87 Safari/537.36',\n",
    "        'Content-Type': 'application/x-www-form-urlencoded',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Cache-Control': 'no-cache',\n",
    "        'Referer': 'http://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236&DB_Short_Name=On-Time',\n",
    "        'Connection': 'keep-alive',\n",
    "        'DNT': '1',\n",
    "    }\n",
    "    os.makedirs('timeseries', exist_ok=True)\n",
    "    data = 'UserTableName=On_Time_Performance&DBShortName=On_Time&RawDataTable=T_ONTIME&sqlstr=+SELECT+FL_DATE%2CUNIQUE_CARRIER%2CCARRIER%2CTAIL_NUM%2CFL_NUM%2CORIGIN%2CDEST%2CCRS_DEP_TIME%2CDEP_TIME%2CTAXI_OUT%2CWHEELS_OFF%2CWHEELS_ON%2CTAXI_IN%2CCRS_ARR_TIME%2CARR_TIME%2CDISTANCE%2CCARRIER_DELAY%2CWEATHER_DELAY%2CNAS_DELAY%2CSECURITY_DELAY%2CLATE_AIRCRAFT_DELAY+FROM++T_ONTIME+WHERE+%28+DEST_STATE_NM%3D%27Illinois%27+OR+ORIGIN_STATE_NM+%3D%27Illinois%27+%29++AND+Month+%3D{month}+AND+YEAR%3D{year}&varlist=FL_DATE%2CUNIQUE_CARRIER%2CCARRIER%2CTAIL_NUM%2CFL_NUM%2CORIGIN%2CDEST%2CCRS_DEP_TIME%2CDEP_TIME%2CTAXI_OUT%2CWHEELS_OFF%2CWHEELS_ON%2CTAXI_IN%2CCRS_ARR_TIME%2CARR_TIME%2CDISTANCE%2CCARRIER_DELAY%2CWEATHER_DELAY%2CNAS_DELAY%2CSECURITY_DELAY%2CLATE_AIRCRAFT_DELAY&grouplist=&suml=&sumRegion=&filter1=title%3D&filter2=title%3D&geo=Illinois&time={month_name}&timename=Month&GEOGRAPHY=Illinois&XYEAR={year}&FREQUENCY={month}&VarDesc=Year&VarType=Num&VarDesc=Quarter&VarType=Num&VarDesc=Month&VarType=Num&VarDesc=DayofMonth&VarType=Num&VarDesc=DayOfWeek&VarType=Num&VarName=FL_DATE&VarDesc=FlightDate&VarType=Char&VarName=UNIQUE_CARRIER&VarDesc=UniqueCarrier&VarType=Char&VarDesc=AirlineID&VarType=Num&VarName=CARRIER&VarDesc=Carrier&VarType=Char&VarName=TAIL_NUM&VarDesc=TailNum&VarType=Char&VarName=FL_NUM&VarDesc=FlightNum&VarType=Char&VarDesc=OriginAirportID&VarType=Num&VarDesc=OriginAirportSeqID&VarType=Num&VarDesc=OriginCityMarketID&VarType=Num&VarName=ORIGIN&VarDesc=Origin&VarType=Char&VarDesc=OriginCityName&VarType=Char&VarDesc=OriginState&VarType=Char&VarDesc=OriginStateFips&VarType=Char&VarDesc=OriginStateName&VarType=Char&VarDesc=OriginWac&VarType=Num&VarDesc=DestAirportID&VarType=Num&VarDesc=DestAirportSeqID&VarType=Num&VarDesc=DestCityMarketID&VarType=Num&VarName=DEST&VarDesc=Dest&VarType=Char&VarDesc=DestCityName&VarType=Char&VarDesc=DestState&VarType=Char&VarDesc=DestStateFips&VarType=Char&VarDesc=DestStateName&VarType=Char&VarDesc=DestWac&VarType=Num&VarName=CRS_DEP_TIME&VarDesc=CRSDepTime&VarType=Char&VarName=DEP_TIME&VarDesc=DepTime&VarType=Char&VarDesc=DepDelay&VarType=Num&VarDesc=DepDelayMinutes&VarType=Num&VarDesc=DepDel15&VarType=Num&VarDesc=DepartureDelayGroups&VarType=Num&VarDesc=DepTimeBlk&VarType=Char&VarName=TAXI_OUT&VarDesc=TaxiOut&VarType=Num&VarName=WHEELS_OFF&VarDesc=WheelsOff&VarType=Char&VarName=WHEELS_ON&VarDesc=WheelsOn&VarType=Char&VarName=TAXI_IN&VarDesc=TaxiIn&VarType=Num&VarName=CRS_ARR_TIME&VarDesc=CRSArrTime&VarType=Char&VarName=ARR_TIME&VarDesc=ArrTime&VarType=Char&VarDesc=ArrDelay&VarType=Num&VarDesc=ArrDelayMinutes&VarType=Num&VarDesc=ArrDel15&VarType=Num&VarDesc=ArrivalDelayGroups&VarType=Num&VarDesc=ArrTimeBlk&VarType=Char&VarDesc=Cancelled&VarType=Num&VarDesc=CancellationCode&VarType=Char&VarDesc=Diverted&VarType=Num&VarDesc=CRSElapsedTime&VarType=Num&VarDesc=ActualElapsedTime&VarType=Num&VarDesc=AirTime&VarType=Num&VarDesc=Flights&VarType=Num&VarName=DISTANCE&VarDesc=Distance&VarType=Num&VarDesc=DistanceGroup&VarType=Num&VarName=CARRIER_DELAY&VarDesc=CarrierDelay&VarType=Num&VarName=WEATHER_DELAY&VarDesc=WeatherDelay&VarType=Num&VarName=NAS_DELAY&VarDesc=NASDelay&VarType=Num&VarName=SECURITY_DELAY&VarDesc=SecurityDelay&VarType=Num&VarName=LATE_AIRCRAFT_DELAY&VarDesc=LateAircraftDelay&VarType=Num&VarDesc=FirstDepTime&VarType=Char&VarDesc=TotalAddGTime&VarType=Num&VarDesc=LongestAddGTime&VarType=Num&VarDesc=DivAirportLandings&VarType=Num&VarDesc=DivReachedDest&VarType=Num&VarDesc=DivActualElapsedTime&VarType=Num&VarDesc=DivArrDelay&VarType=Num&VarDesc=DivDistance&VarType=Num&VarDesc=Div1Airport&VarType=Char&VarDesc=Div1AirportID&VarType=Num&VarDesc=Div1AirportSeqID&VarType=Num&VarDesc=Div1WheelsOn&VarType=Char&VarDesc=Div1TotalGTime&VarType=Num&VarDesc=Div1LongestGTime&VarType=Num&VarDesc=Div1WheelsOff&VarType=Char&VarDesc=Div1TailNum&VarType=Char&VarDesc=Div2Airport&VarType=Char&VarDesc=Div2AirportID&VarType=Num&VarDesc=Div2AirportSeqID&VarType=Num&VarDesc=Div2WheelsOn&VarType=Char&VarDesc=Div2TotalGTime&VarType=Num&VarDesc=Div2LongestGTime&VarType=Num&VarDesc=Div2WheelsOff&VarType=Char&VarDesc=Div2TailNum&VarType=Char&VarDesc=Div3Airport&VarType=Char&VarDesc=Div3AirportID&VarType=Num&VarDesc=Div3AirportSeqID&VarType=Num&VarDesc=Div3WheelsOn&VarType=Char&VarDesc=Div3TotalGTime&VarType=Num&VarDesc=Div3LongestGTime&VarType=Num&VarDesc=Div3WheelsOff&VarType=Char&VarDesc=Div3TailNum&VarType=Char&VarDesc=Div4Airport&VarType=Char&VarDesc=Div4AirportID&VarType=Num&VarDesc=Div4AirportSeqID&VarType=Num&VarDesc=Div4WheelsOn&VarType=Char&VarDesc=Div4TotalGTime&VarType=Num&VarDesc=Div4LongestGTime&VarType=Num&VarDesc=Div4WheelsOff&VarType=Char&VarDesc=Div4TailNum&VarType=Char&VarDesc=Div5Airport&VarType=Char&VarDesc=Div5AirportID&VarType=Num&VarDesc=Div5AirportSeqID&VarType=Num&VarDesc=Div5WheelsOn&VarType=Char&VarDesc=Div5TotalGTime&VarType=Num&VarDesc=Div5LongestGTime&VarType=Num&VarDesc=Div5WheelsOff&VarType=Char&VarDesc=Div5TailNum&VarType=Char'\n",
    "\n",
    "    r = requests.post('http://www.transtats.bts.gov/DownLoad_Table.asp?Table_ID=236&Has_Group=3&Is_Zipped=0',\n",
    "                      headers=headers, data=data.format(year=year, month=month, month_name=month_name),\n",
    "                      stream=True)\n",
    "    fp = os.path.join('timeseries', '{}-{}.zip'.format(year, month))\n",
    "\n",
    "    with open(fp, 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=1024): \n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    return fp\n",
    "    \n",
    "def download_many(start, end):\n",
    "    months = pd.date_range(start, end=end, freq='M')    \n",
    "    # We could easily parallelize this loop.\n",
    "    for i, month in enumerate(months):\n",
    "        download_one(month)\n",
    "    \n",
    "def unzip_one(fp):\n",
    "    zf = zipfile.ZipFile(fp)\n",
    "    csv = zf.extract(zf.filelist[0])\n",
    "    return csv\n",
    "\n",
    "def time_to_datetime(df, columns):\n",
    "    '''\n",
    "    Combine all time items into datetimes.\n",
    "    \n",
    "    2014-01-01,1149.0 -> 2014-01-01T11:49:00\n",
    "    '''\n",
    "    def converter(col):\n",
    "        timepart = (col.astype(str)\n",
    "                       .str.replace('\\.0$', '')  # NaNs force float dtype\n",
    "                       .str.pad(4, fillchar='0'))\n",
    "        return  pd.to_datetime(df['fl_date'] + ' ' +\n",
    "                               timepart.str.slice(0, 2) + ':' +\n",
    "                               timepart.str.slice(2, 4),\n",
    "                               errors='coerce')\n",
    "        return datetime_part\n",
    "    df[columns] = df[columns].apply(converter)\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_one(fp):\n",
    "    df = (pd.read_csv(fp, encoding='latin1')\n",
    "            .rename(columns=str.lower)\n",
    "            .drop('unnamed: 21', axis=1)\n",
    "            .pipe(time_to_datetime, ['dep_time', 'arr_time', 'crs_arr_time',\n",
    "                                     'crs_dep_time'])\n",
    "            .assign(fl_date=lambda x: pd.to_datetime(x['fl_date'])))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "download_many('2000-01-01', '2016-01-01')\n",
    "\n",
    "zips = glob.glob(os.path.join('timeseries', '*.zip'))\n",
    "csvs = [unzip_one(fp) for fp in zips]\n",
    "dfs = [read_one(fp) for fp in csvs]\n",
    "df = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', 100):\n",
    "    print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_cols = ['unique_carrier', 'carrier', 'tail_num', 'origin', 'dest']\n",
    "\n",
    "df[cat_cols] = df[cat_cols].apply(pd.Categorical)\n",
    "\n",
    "df.to_hdf('ts.hdf5', 'ts', format='table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_hdf('ts.hdf5', 'ts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the historical values with a resample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily = df.fl_date.value_counts().sort_index()\n",
    "y = daily.resample('MS').mean()\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I use the `\"MS\"` frequency code there.\n",
    "Pandas defaults to end of month (or end of year).\n",
    "Append an `'S'` to get the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.plot()\n",
    "sns.despine()\n",
    "plt.savefig('../output/images/ts-y.svg', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think back to a typical regression problem, ignoring anything to do wtih time series for now.\n",
    "The usual task is to predict some value $y$ using some a linear combination of features in $X$.\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p + \\epsilon$$\n",
    "\n",
    "When working with time series, some of the most important (and sometimes *only*) features are the previous, or *lagged*, values of $y$.\n",
    "\n",
    "We'll start by doing just that: running a regression of `y` on lagged values of itself.\n",
    "We'll see that this regression suffers from a few problems: multicolinearity, autocorrelation, non-stationarity, and seasonality.\n",
    "Once we touch on each of those problems, we'll use a second model, seasonal ARIMA, which handles those problems for us.\n",
    "\n",
    "First, let's create a dataframe with our lagged values of `y` using the `.shift` method, which shifts the index `i` periods, so it lines up with that observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (pd.concat([y.shift(i) for i in range(6)], axis=1,\n",
    "               keys=['y'] + ['L%s' % i for i in range(1, 6)])\n",
    "       .dropna())\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fit the lagged model using statsmodels (which uses [patsy](http://patsy.readthedocs.org) to translate the formula string to a design matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_lagged = smf.ols('y ~ trend + L1 + L2 + L3 + L4 + L5',\n",
    "                     data=X.assign(trend=np.arange(len(X))))\n",
    "res_lagged = mod_lagged.fit()\n",
    "res_lagged.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few problems with this approach though.\n",
    "Since our lagged values are highly correlated with each other, our regression suffers from multicollinearity.\n",
    "That ruins our estimates of the slopes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(X.corr())\n",
    "plt.savefig('../output/images/ts-corr.svg', transparent=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seoncd, we'd intuitively expect the $\\beta_i$s to gradually decline to zero.\n",
    "The immediately preceding period should be most important ($\\beta_1$ is the largest coefficient in absolute value), followed by $\\beta_2$, and $\\beta_3$...\n",
    "Looking at the regression summary and the bar graph below, this isn't the case (the cause is related to multicolinearity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_lagged.params.drop(['Intercept', 'trend']).plot.bar(rot=0)\n",
    "plt.ylabel('Coefficeint')\n",
    "sns.despine()\n",
    "plt.savefig('../output/images/ts-lagged-coef.svg', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, our degrees of freedom drop since we lose two for each variable (one for estimating the coefficient, one for the lost observation as a result of the `shift`).\n",
    "At least in (macro)econometrics, each observation is precious and we're loath to throw them away, though sometimes that's unavoidable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another problem our lagged model suffered from is autocorrelation (also know as serial correlation).\n",
    "Roughly speaking, autocorrelation is when there's a clear pattern in the residuals.\n",
    "Let's fit a simple model of $y = \\beta_0 + \\beta_1 T + \\epsilon$, where `T` is the time trend (`np.arange(len(y))`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `Results.resid` is a Series of residuals: y - ŷ\n",
    "mod_trend = sm.OLS.from_formula(\n",
    "    'y ~ trend', data=y.to_frame(name='y')\n",
    "                       .assign(trend=np.arange(len(y))))\n",
    "res_trend = mod_trend.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residuals (the observed minus the expected, or $\\hat{e_t} = y_t - \\hat{y_t}$) are supposed to be [white noise](https://en.wikipedia.org/wiki/White_noise).\n",
    "That's [one of the assumptions](https://en.wikipedia.org/wiki/Gauss–Markov_theorem) many of the properties of linear regression are founded upon.\n",
    "In this case there's a correlation between one residual and the next: if the residual at time $t$ was above expecation, then the residual at time $t + 1$ is *much* more likely to be above average as well ($e_t > 0 \\implies E_t[e_{t+1}] > 0$).\n",
    "\n",
    "We'll define a helper function to plot the residuals time series, and some diagnostics about them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tsplot(y, lags=None, figsize=(10, 8)):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    layout = (2, 2)\n",
    "    ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n",
    "    acf_ax = plt.subplot2grid(layout, (1, 0))\n",
    "    pacf_ax = plt.subplot2grid(layout, (1, 1))\n",
    "    \n",
    "    y.plot(ax=ts_ax)\n",
    "    smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n",
    "    smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n",
    "    [ax.set_xlim(1.5) for ax in [acf_ax, pacf_ax]]\n",
    "    sns.despine()\n",
    "    plt.tight_layout()\n",
    "    return ts_ax, acf_ax, pacf_ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsplot(res_trend.resid, lags=36)\n",
    "plt.savefig('../output/images/ts-res-trend-tsplot.svg', transparent=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top subplot shows the time series of our residuals $e_t$.\n",
    "The bottom left is the [autocorrelation](https://www.otexts.org/fpp/2/2#autocorrelation) of the residuals.\n",
    "It measures the correlation between a a value and it's lagged self, e.g. $corr(e_t, e_{t-1}), corr(e_t, e_{t-2}), \\ldots$.\n",
    "I won't really go into partial autocorrelation, but it's a similar concept.\n",
    "\n",
    "Autocorelation is a problem in regular regressions like above, but we'll use it to our advantage when we setup an ARIMA model below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationarity\n",
    "\n",
    "It's important that your dataset be stationary, otherwise you run the risk of finding [spurious correlations](http://www.tylervigen.com/spurious-correlations).\n",
    "Granger and Newbold (1974) as well.\n",
    "The typical way to handle non-stationarity is to difference the non-stationary variable until is is stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.to_frame(name='y').assign(Δy=lambda x: x.y.diff()).plot(subplots=True)\n",
    "sns.despine()\n",
    "plt.savefig('../output/images/ts-y-deltay.svg', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our original series actually doesn't look *that* bad.\n",
    "But we have more rigorous methods for detetcing whether a series is non-stationary.\n",
    "One example is the Augmented Dickey-Fuller test.\n",
    "It's a statistical hypothesis test that says:\n",
    "\n",
    "$H_0$ (null hypothesis): $y$ is non-stationary\n",
    "\n",
    "$H_A$ (alternative hypothesis): $y$ is stationary\n",
    "\n",
    "\n",
    "$$y_t^\\prime = \\phi y_{t-1} + \\beta_1 y_{t-1}^\\prime + \\beta_2 y_{t-2}^\\prime + \\ldots + \\beta_k y_{t-k}^\\prime$$\n",
    "\n",
    "This is implemented in statsmodels as `smt.adfuller`. The return type is a bit busy for me, so we'll wrap it in a `namedtuple`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "ADF = namedtuple(\"ADF\", \"adf pvalue usedlag nobs critical icbest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADF(*smt.adfuller(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we failed to reject the null hypothesis that the original series was non-stationary.\n",
    "Let's difference it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADF(*smt.adfuller(y.diff().dropna()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks better.\n",
    "It's not statistically significant at the 5% level, but who cares what statisticins say anyway.\n",
    "\n",
    "We'll fit another OLS model of $\\Delta y = \\beta_0 + \\beta_1 L \\Delta y_{t-1} + e_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (y.to_frame(name='y')\n",
    "         .assign(Δy=lambda df: df.y.diff())\n",
    "         .assign(LΔy=lambda df: df.Δy.shift()))\n",
    "mod_stationary = smf.ols('Δy ~ LΔy', data=data.dropna())\n",
    "res_stationary = mod_stationary.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsplot(res_stationary.resid, lags=24)\n",
    "plt.savefig('../output/images/ts-stationary.svg', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've taken care of multicolinearity, autocorelation, and stationarity, but we still aren't done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasonality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our last issue to deal with is seasonality: we have strong monthly seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt.seasonal_decompose(y).plot()\n",
    "plt.savefig('../output/images/ts-decompose.svg', transparent=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few ways to handle seasonality. We'll just rely on the `SARIMAX` method to do it for us. For now, recognize that it's a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA\n",
    "\n",
    "So, we've sketched the problems with regular old regression: multicolinearity, autocorrelation, non-stationarity, and seasonality.\n",
    "Our tool of choice, `smt.SARIMAX`, which stands for Seasonal ARIMA with eXogenous regressors, can handle all these.\n",
    "We'll walk through the components in pieces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARIMA stands for AutoRegressive Integrated Moving Average, and it's a relatively simple way of modeling univariate time series.\n",
    "It's made up of three components, and is typically written as $\\mathrm{ARIMA}(p, d, q)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [AutoRegressive](https://www.otexts.org/fpp/8/3)\n",
    "\n",
    "The idea is to predict a variable by a linear combination of its lagged values (*auto*-regressive as in regressing a value on its past *self*).\n",
    "An AR(p), where $p$ repgresents the number of past values used, is written as\n",
    "\n",
    "$$y_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\ldots + \\phi_p y_{t-p} + e_t$$\n",
    "\n",
    "$c$ is a constant and $e_t$ is white noise. Other than that this is quite similar to a linear regression model with multiple predictors, but the predictors happen to be lagged values of $y$ (though they are estimated differently)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrated\n",
    "\n",
    "Integrated is like the opposite of differencing, and is the part that deals with stationarity.\n",
    "If you have to difference your dataset 1 time to get it stationary, then $d=1$.\n",
    "We'll introduce one bit of notation for differencing: $\\Delta y_t = y_t - y_{t-1}$ for $d=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Moving Average](https://www.otexts.org/fpp/8/4)\n",
    "\n",
    "MA models look somewhat similar to the AR component, but it's dealing with different values.\n",
    "\n",
    "$$y_t = c + e_t + \\theta_1 e_{t-1} + \\theta_2 e_{t-2} + \\ldots + \\theta_q e_{t-q}$$\n",
    "\n",
    "$c$ again is a constant and $e_t$ again is white noise.\n",
    "But now the coefficients are the *residuals* from previous predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining\n",
    "\n",
    "Putting that together, we have an ARIMA(1, 1, 1) proces is written as\n",
    "\n",
    "$$\\Delta y_t = c + \\phi_1 \\Delta y_{t-1} + \\theta_t e_{t-1} + e_t$$ \n",
    "\n",
    "Using *lag notation*, where $L y_t = y_{t-1}$, i.e. `y.shift()` in pandas, we can rewrite that as\n",
    "\n",
    "$$(1 - \\phi_1 L) (1 - L)y_t = c + (1 + \\theta L)e_t$$\n",
    "\n",
    "For our ARIMA(1, 1, 1) model. In general that becomes\n",
    "\n",
    "$$(1 - \\phi_1 L - \\ldots - \\phi_p L^p) (1 - L)^d y_t = c + (1 + \\theta L + \\ldots + \\theta_q L^q)e_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = smt.SARIMAX(y, trend='c', order=(1, 1, 1))\n",
    "res = mod.fit()\n",
    "tsplot(res.resid[2:], lags=24)\n",
    "plt.savefig('../output/images/ts-arima.svg', transparent=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things are looking better, but we still haven't accounted for seasonality.\n",
    "\n",
    "A seasonal ARIMA model is written as $\\mathrm{ARIMA}(p,d,q)×(P,D,Q)_s$.\n",
    "Lowercase letters are for the non-seasonal component, just like before. Upper-case letters are a similar specification for the seasonal component, where $s$ is the periodicity (4 for quarterly, 12 for monthly).\n",
    "\n",
    "It's like we have two processes, one for non-seasonal component and one for seasonal components, and we mulitply them together.\n",
    "\n",
    "The general form of that looks like (quoting the statsmodels docs here)\n",
    "\n",
    "$$\\phi_p(L)\\tilde{\\phi}_P(L^S)\\Delta^d\\Delta_s^D y_t = A(t) + \\theta_q(L)\\tilde{\\theta}_Q(L^s)e_t$$\n",
    "\n",
    "where\n",
    "\n",
    "- $\\phi_p(L)$ is the non-seasonal autoregressive lag polynomial\n",
    "- $\\tilde{\\phi}_P(L^S)$ is the seasonal autoregressive lag polynomial\n",
    "- $\\Delta^d\\Delta_s^D$ is the time series, differenced  $d$ times, and seasonally differenced $D$ times.\n",
    "- $A(t)$ is the trend polynomial (including the intercept)\n",
    "- $\\theta_q(L)$ is the non-seasonal moving average lag polynomial\n",
    "- $\\tilde{\\theta}_Q(L^s)$  is the seasonal moving average lag polynomial\n",
    "\n",
    "I don't find that to be very clear, but maybe an example will help. We'll fit a seasonal ARIMA$(2,0,1)×(1, 1, 1)_{12}$.\n",
    "\n",
    "So the nonseasonal component is\n",
    "\n",
    "- $p=2$: period autoregressive: use $y_{t-1}$ and $y_{t-2}$\n",
    "- $d=0$: no first-differencing of the data\n",
    "- $q=1$: use the previous non-seasonal residual, $e_{t-1}$, to forecast\n",
    "\n",
    "And the seasonal component is\n",
    "\n",
    "- $P=1$: use the previous seasonal value: $y_{t-12}$\n",
    "- $D=1$: Difference the series 12 periods back: `y.diff(12)`\n",
    "- $Q=1$: Use the previous seasonal residual, $e{t-12}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_seasonal = smt.SARIMAX(y, trend='c',\n",
    "                           order=(1, 1, 2), seasonal_order=(0, 1, 2, 12),\n",
    "                           simple_differencing=False)\n",
    "res_seasonal = mod_seasonal.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_seasonal.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsplot(res_seasonal.resid[12:], lags=24)\n",
    "plt.savefig('../output/images/ts-seasonal.svg', transparent=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things look much better now.\n",
    "\n",
    "One thing I didn't really talk about is order selection. How to choose $p, d, q, P, D$ and $Q$.\n",
    "R's forecast package does have a handy `auto.arima` function that does this for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the results to make one-step-ahead forecasts.\n",
    "At each point, we take the history up to that point and make a forecast for the next month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = res_seasonal.get_prediction(start='2001-03-01')\n",
    "pred_ci = pred.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = y.plot(label='observed')\n",
    "pred.predicted_mean.plot(ax=ax, label='Forecast', alpha=.7)\n",
    "ax.fill_between(pred_ci.index,\n",
    "                pred_ci.iloc[:, 0],\n",
    "                pred_ci.iloc[:, 1], color='k', alpha=.2)\n",
    "plt.legend()\n",
    "sns.despine()\n",
    "plt.savefig('../output/images/ts-one-step.svg', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternativly, we can make *dynamic* forcasts as of some month (January 2013 in the example below).\n",
    "That means the forecast from that point foreward only use inforamtion available as of January 2013.\n",
    "In this case, the predictions are generated in a simlar way: a bunch of one-step forecasts.\n",
    "Only instead of plugging in the *actual* past values, we plug in the *forecast* past values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dy = res_seasonal.get_prediction(start='2002-03-01', dynamic='2013-01-01')\n",
    "pred_dy_ci = pred_dy.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = y.plot(label='observed')\n",
    "pred_dy.predicted_mean.plot(ax=ax, label='Forecast')\n",
    "ax.fill_between(pred_dy_ci.index,\n",
    "                pred_dy_ci.iloc[:, 0],\n",
    "                pred_dy_ci.iloc[:, 1], color='k', alpha=.25)\n",
    "plt.legend()\n",
    "sns.despine()\n",
    "plt.savefig('../output/images/ts-dynamic.svg', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
