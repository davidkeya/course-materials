{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    " \n",
    "# Ensembles and Random Forests\n",
    " \n",
    "_Author: Joseph Nelson (DC)_\n",
    "\n",
    "*Adapted from Chapter 8 of [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "Students will be able to:\n",
    "\n",
    "- Understand how and why decision trees can be improved using bagging and random forests.\n",
    "- Build random forest models for classification and regression.\n",
    "- Know how to extract the most important predictors in a random forest model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Guide\n",
    "- [Introduction](#introduction)\n",
    "- [Part 1: Manual Ensembling](#part-one)\n",
    "- [Part 2: Bagging](#part-two)\n",
    "    - [Manually Implementing Bagged Decision Trees](#manual-bagged)\n",
    "    - [Bagged Decision Trees in `scikit-learn`](#manual-sklearn)\n",
    "    - [Estimating Out-of-Sample Error](#oos-error)\n",
    "    \n",
    "    \n",
    "- [Part 3: Random Forests](#part-three)\n",
    "- [Part 4: Building and Tuning Decision Trees and Random Forests](#part-four)\n",
    "    - [Optional: Predicting Salary With a Decision Tree](#decision-tree)\n",
    "    - [Predicting Salary With a Random Forest](#random-forest-demo)\n",
    "    - [Comparing Random Forests With Decision Trees](#comparing)\n",
    "    \n",
    "    \n",
    "- [Optional: Tuning Individual Parameters](#tuning)\n",
    "- [Summary](#summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"introduction\"></a>\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Ensembling?\n",
    "\n",
    "**Ensemble learning (or \"ensembling\")** is the process of combining several predictive models in order to produce a combined model that is more accurate than any individual model. For example, given predictions from several models we could:\n",
    "\n",
    "- **Regression:** Take the average of the predictions.\n",
    "- **Classification:** Take a vote and use the most common prediction.\n",
    "\n",
    "For ensembling to work well, the models must be:\n",
    "\n",
    "- **Accurate:** They outperform the null model.\n",
    "- **Independent:** Their predictions are generated using different processes.\n",
    "\n",
    "**The big idea:** If you have a collection of individually imperfect (and independent) models, the \"one-off\" mistakes made by each model are probably not going to be made by the rest of the models, and thus the mistakes will be discarded when you average the models.\n",
    "\n",
    "There are two basic **methods for ensembling:**\n",
    "\n",
    "- Manually ensembling your individual models.\n",
    "- Using a model that ensembles for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part-one\"></a>\n",
    "## Part 1: Manual Ensembling\n",
    "\n",
    "What makes an effective manual ensemble?\n",
    "\n",
    "- Different types of **models**.\n",
    "- Different combinations of **features**.\n",
    "- Different **tuning parameters**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Machine learning flowchart](assets/crowdflower_ensembling.jpg)\n",
    "\n",
    "*Machine learning flowchart created by the [winner](https://github.com/ChenglongChen/Kaggle_CrowdFlower) of Kaggle's [CrowdFlower competition](https://www.kaggle.com/c/crowdflower-search-relevance)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Manual Ensembling With a Single Model Approach\n",
    "\n",
    "**Advantages of manual ensembling:**\n",
    "\n",
    "- It increases predictive accuracy.\n",
    "- It's easy to get started.\n",
    "\n",
    "**Disadvantages of manual ensembling:**\n",
    "\n",
    "- It decreases interpretability.\n",
    "- It takes longer to train.\n",
    "- It takes longer to predict.\n",
    "- It is more complex to automate and maintain.\n",
    "- Small gains in accuracy may not be worth the added complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part-two\"></a>\n",
    "## Part 2: Bagging\n",
    "\n",
    "The primary weakness of **decision trees** is that they don't tend to have the best predictive accuracy. This is partially because of **high variance**, meaning that different splits in the training data can lead to very different trees.\n",
    "\n",
    "**Bagging** is a general-purpose procedure for reducing the variance of a machine learning method but is particularly useful for decision trees. Bagging is short for **bootstrap aggregation**, meaning the aggregation of bootstrap samples.\n",
    "\n",
    "A **bootstrap sample** is a random sample with replacement. So, it has the same size as the original sample but might duplicate some of the original observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "[ 6 12 13  9 10 12  6 16  1 17  2 13  8 14  7 19  6 19 12 11]\n"
     ]
    }
   ],
   "source": [
    "# Set a seed for reproducibility.\n",
    "np.random.seed(1)\n",
    "\n",
    "# Create an array of 1 through 20.\n",
    "nums = np.arange(1, 21)\n",
    "print(nums)\n",
    "\n",
    "# Sample that array 20 times with replacement.\n",
    "print(np.random.choice(a=nums, size=20, replace=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does bagging work (for decision trees)?**\n",
    "\n",
    "1. Grow B trees using B bootstrap samples from the training data.\n",
    "2. Train each tree on its bootstrap sample and make predictions.\n",
    "3. Combine the predictions:\n",
    "    - Average the predictions for **regression trees**.\n",
    "    - Take a vote for **classification trees**.\n",
    "\n",
    "Notes:\n",
    "\n",
    "- **Each bootstrap sample** should be the same size as the original training set. (It may contain repeated rows.)\n",
    "- **B** should be a large enough value that the error seems to have \"stabilized\".\n",
    "- The trees are **grown deep** so that they have low bias/high variance.\n",
    "\n",
    "Bagging increases predictive accuracy by **reducing the variance**, similar to how cross-validation reduces the variance associated with train/test split (for estimating out-of-sample error) by splitting many times an averaging the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"manual-bagged\"></a>\n",
    "## Manually Implementing Bagged Decision Trees (with B=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>miles</th>\n",
       "      <th>doors</th>\n",
       "      <th>vtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22000</td>\n",
       "      <td>2012</td>\n",
       "      <td>13000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14000</td>\n",
       "      <td>2010</td>\n",
       "      <td>30000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13000</td>\n",
       "      <td>2010</td>\n",
       "      <td>73500</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9500</td>\n",
       "      <td>2009</td>\n",
       "      <td>78000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9000</td>\n",
       "      <td>2007</td>\n",
       "      <td>47000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4000</td>\n",
       "      <td>2006</td>\n",
       "      <td>124000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3000</td>\n",
       "      <td>2004</td>\n",
       "      <td>177000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2000</td>\n",
       "      <td>2004</td>\n",
       "      <td>209000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3000</td>\n",
       "      <td>2003</td>\n",
       "      <td>138000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1900</td>\n",
       "      <td>2003</td>\n",
       "      <td>160000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2500</td>\n",
       "      <td>2003</td>\n",
       "      <td>190000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5000</td>\n",
       "      <td>2001</td>\n",
       "      <td>62000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1800</td>\n",
       "      <td>1999</td>\n",
       "      <td>163000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1300</td>\n",
       "      <td>1997</td>\n",
       "      <td>138000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    price  year   miles  doors  vtype\n",
       "0   22000  2012   13000      2      0\n",
       "1   14000  2010   30000      2      0\n",
       "2   13000  2010   73500      4      0\n",
       "3    9500  2009   78000      4      0\n",
       "4    9000  2007   47000      4      0\n",
       "5    4000  2006  124000      2      0\n",
       "6    3000  2004  177000      4      0\n",
       "7    2000  2004  209000      4      1\n",
       "8    3000  2003  138000      2      0\n",
       "9    1900  2003  160000      4      0\n",
       "10   2500  2003  190000      2      1\n",
       "11   5000  2001   62000      4      0\n",
       "12   1800  1999  163000      2      1\n",
       "13   1300  1997  138000      4      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in and prepare the vehicle training data.\n",
    "import pandas as pd\n",
    "\n",
    "path = './data/vehicles_train.csv'\n",
    "train = pd.read_csv(path)\n",
    "train['vtype'] = train.vtype.map({'car':0, 'truck':1})\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([13,  2, 12,  2,  6,  1,  3, 10, 11,  9,  6,  1,  0,  1]),\n",
       " array([ 9,  0,  0,  9,  3, 13,  4,  0,  0,  4,  1,  7,  3,  2]),\n",
       " array([ 4,  7,  2,  4,  8, 13,  0,  7,  9,  3, 12, 12,  4,  6]),\n",
       " array([ 1,  5,  6, 11,  2,  1, 12,  8,  3, 10,  5,  0, 11,  2]),\n",
       " array([10, 10,  6, 13,  2,  4, 11, 11, 13, 12,  4,  6, 13,  3]),\n",
       " array([10,  0,  6,  4,  7, 11,  6,  7,  1, 11, 10,  5,  7,  9]),\n",
       " array([ 2,  4,  8,  1, 12,  2,  1,  1,  3, 12,  5,  9,  0,  8]),\n",
       " array([11,  1,  6,  3,  3, 11,  5,  9,  7,  9,  2,  3, 11,  3]),\n",
       " array([ 3,  8,  6,  9,  7,  6,  3,  9,  6, 12,  6, 11,  6,  1]),\n",
       " array([13, 10,  3,  4,  3,  1, 13,  0,  5,  8, 13,  6, 11,  8])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set a seed for reproducibility.\n",
    "np.random.seed(123)\n",
    "\n",
    "# Create ten bootstrap samples (which will be used to select rows from the DataFrame).\n",
    "samples = [np.random.choice(a=14, size=14, replace=True) for _ in range(1, 11)]\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>miles</th>\n",
       "      <th>doors</th>\n",
       "      <th>vtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1300</td>\n",
       "      <td>1997</td>\n",
       "      <td>138000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13000</td>\n",
       "      <td>2010</td>\n",
       "      <td>73500</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1800</td>\n",
       "      <td>1999</td>\n",
       "      <td>163000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13000</td>\n",
       "      <td>2010</td>\n",
       "      <td>73500</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3000</td>\n",
       "      <td>2004</td>\n",
       "      <td>177000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14000</td>\n",
       "      <td>2010</td>\n",
       "      <td>30000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9500</td>\n",
       "      <td>2009</td>\n",
       "      <td>78000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2500</td>\n",
       "      <td>2003</td>\n",
       "      <td>190000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5000</td>\n",
       "      <td>2001</td>\n",
       "      <td>62000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1900</td>\n",
       "      <td>2003</td>\n",
       "      <td>160000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3000</td>\n",
       "      <td>2004</td>\n",
       "      <td>177000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14000</td>\n",
       "      <td>2010</td>\n",
       "      <td>30000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22000</td>\n",
       "      <td>2012</td>\n",
       "      <td>13000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14000</td>\n",
       "      <td>2010</td>\n",
       "      <td>30000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    price  year   miles  doors  vtype\n",
       "13   1300  1997  138000      4      0\n",
       "2   13000  2010   73500      4      0\n",
       "12   1800  1999  163000      2      1\n",
       "2   13000  2010   73500      4      0\n",
       "6    3000  2004  177000      4      0\n",
       "1   14000  2010   30000      2      0\n",
       "3    9500  2009   78000      4      0\n",
       "10   2500  2003  190000      2      1\n",
       "11   5000  2001   62000      4      0\n",
       "9    1900  2003  160000      4      0\n",
       "6    3000  2004  177000      4      0\n",
       "1   14000  2010   30000      2      0\n",
       "0   22000  2012   13000      2      0\n",
       "1   14000  2010   30000      2      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the rows for the first decision tree.\n",
    "train.iloc[samples[0], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>miles</th>\n",
       "      <th>doors</th>\n",
       "      <th>vtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3000</td>\n",
       "      <td>2003</td>\n",
       "      <td>130000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6000</td>\n",
       "      <td>2005</td>\n",
       "      <td>82500</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12000</td>\n",
       "      <td>2010</td>\n",
       "      <td>60000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price  year   miles  doors  vtype\n",
       "0   3000  2003  130000      4      1\n",
       "1   6000  2005   82500      4      0\n",
       "2  12000  2010   60000      2      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in and prepare the vehicle testing data.\n",
    "path = './data/vehicles_test.csv'\n",
    "test = pd.read_csv(path)\n",
    "test['vtype'] = test.vtype.map({'car':0, 'truck':1})\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1300.,   5000.,  14000.],\n",
       "       [  1300.,   1300.,  13000.],\n",
       "       [  3000.,   3000.,  13000.],\n",
       "       [  4000.,   5000.,  13000.],\n",
       "       [  1300.,   5000.,  13000.],\n",
       "       [  4000.,   5000.,  14000.],\n",
       "       [  4000.,   4000.,  13000.],\n",
       "       [  4000.,   5000.,  13000.],\n",
       "       [  3000.,   5000.,   9500.],\n",
       "       [  4000.,   5000.,   9000.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Grow each tree deep.\n",
    "treereg = DecisionTreeRegressor(max_depth=None, random_state=123)\n",
    "\n",
    "# List for storing predicted price from each tree:\n",
    "predictions = []\n",
    "\n",
    "# Define testing data.\n",
    "X_test = test.iloc[:, 1:]\n",
    "y_test = test.iloc[:, 0]\n",
    "\n",
    "# Grow one tree for each bootstrap sample and make predictions on testing data.\n",
    "for sample in samples:\n",
    "    X_train = train.iloc[sample, 1:]\n",
    "    y_train = train.iloc[sample, 0]\n",
    "    treereg.fit(X_train, y_train)\n",
    "    y_pred = treereg.predict(X_test)\n",
    "    predictions.append(y_pred)\n",
    "\n",
    "# Convert predictions from list to NumPy array.\n",
    "predictions = np.array(predictions)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2990.,   4330.,  12450.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average predictions.\n",
    "np.mean(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "998.58232843700307"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate RMSE.\n",
    "from sklearn import metrics\n",
    "y_pred = np.mean(predictions, axis=0)\n",
    "np.sqrt(metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"manual-sklearn\"></a>\n",
    "## Bagged Decision Trees in `scikit-learn` (with B=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the training and testing sets.\n",
    "X_train = train.iloc[:, 1:]\n",
    "y_train = train.iloc[:, 0]\n",
    "X_test = test.iloc[:, 1:]\n",
    "y_test = test.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instruct BaggingRegressor to use DecisionTreeRegressor as the \"base estimator.\"\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "bagreg = BaggingRegressor(DecisionTreeRegressor(), n_estimators=500, bootstrap=True, oob_score=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3344.2,   5395. ,  12902. ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit and predict.\n",
    "bagreg.fit(X_train, y_train)\n",
    "y_pred = bagreg.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "657.80003040437748"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate RMSE.\n",
    "np.sqrt(metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"oos-error\"></a>\n",
    "## Estimating Out-of-Sample Error\n",
    "\n",
    "For bagged models, out-of-sample error can be estimated without using **train/test split** or **cross-validation**!\n",
    "\n",
    "For each tree, the **unused observations** are called \"out-of-bag\" observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13,  2, 12,  2,  6,  1,  3, 10, 11,  9,  6,  1,  0,  1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the first bootstrap sample.\n",
    "samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([0, 1, 2, 3, 6, 9, 10, 11, 12, 13])\n",
      "set([0, 1, 2, 3, 4, 7, 9, 13])\n",
      "set([0, 2, 3, 4, 6, 7, 8, 9, 12, 13])\n",
      "set([0, 1, 2, 3, 5, 6, 8, 10, 11, 12])\n",
      "set([2, 3, 4, 6, 10, 11, 12, 13])\n",
      "set([0, 1, 4, 5, 6, 7, 9, 10, 11])\n",
      "set([0, 1, 2, 3, 4, 5, 8, 9, 12])\n",
      "set([1, 2, 3, 5, 6, 7, 9, 11])\n",
      "set([1, 3, 6, 7, 8, 9, 11, 12])\n",
      "set([0, 1, 3, 4, 5, 6, 8, 10, 11, 13])\n"
     ]
    }
   ],
   "source": [
    "# Show the \"in-bag\" observations for each sample.\n",
    "for sample in samples:\n",
    "    print(set(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 5, 7, 8]\n",
      "[5, 6, 8, 10, 11, 12]\n",
      "[1, 5, 10, 11]\n",
      "[4, 7, 9, 13]\n",
      "[0, 1, 5, 7, 8, 9]\n",
      "[2, 3, 8, 12, 13]\n",
      "[6, 7, 10, 11, 13]\n",
      "[0, 4, 8, 10, 12, 13]\n",
      "[0, 2, 4, 5, 10, 13]\n",
      "[2, 7, 9, 12]\n"
     ]
    }
   ],
   "source": [
    "# Show the \"out-of-bag\" observations for each sample.\n",
    "for sample in samples:\n",
    "    print(sorted(set(range(14)) - set(sample)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculating \"out-of-bag error:\"**\n",
    "\n",
    "1. For each observation in the training data, predict its response value using **only** the trees in which that observation was out-of-bag. Average those predictions (for regression) or take a vote (for classification).\n",
    "2. Compare all predictions to the actual response values in order to compute the out-of-bag error.\n",
    "\n",
    "When B is sufficiently large, the **out-of-bag error** is an accurate estimate of **out-of-sample error**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79869551339899825"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the out-of-bag R-squared score (not MSE, unfortunately) for B=500.\n",
    "bagreg.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating Feature Importance\n",
    "\n",
    "Bagging increases **predictive accuracy** but decreases **model interpretability** because it's no longer possible to visualize the tree to understand the importance of each feature.\n",
    "\n",
    "However, we can still obtain an overall summary of **feature importance** from bagged models:\n",
    "\n",
    "- **Bagged regression trees:** Calculate the total amount that **MSE** decreases due to splits over a given feature, averaged over all trees\n",
    "- **Bagged classification trees:** Calculate the total amount that **Gini index** decreases due to splits over a given feature, averaged over all trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part-three\"></a>\n",
    "## Part 3: Random Forests\n",
    "\n",
    "Random Forests offer a **slight variation on bagged trees** with even better performance:\n",
    "\n",
    "- Exactly like bagging, we create an ensemble of decision trees using bootstrapped samples of the training set.\n",
    "- However, when building each tree, each time a split is considered, a **random sample of m features** is chosen as split candidates from the **full set of p features**. The split is only allowed to use **one of those m features**.\n",
    "    - A new random sample of features is chosen for **every single tree at every single split**.\n",
    "    - For **classification**, m is typically chosen to be the square root of p.\n",
    "    - For **regression**, m is typically chosen to be somewhere between p/3 and p.\n",
    "\n",
    "What's the point?\n",
    "\n",
    "- Suppose there is **one very strong feature** in the data set. When using bagged trees, most of the trees will use that feature as the top split, resulting in an ensemble of similar trees that are **highly correlated**.\n",
    "- Averaging highly correlated quantities does not significantly reduce variance (which is the entire goal of bagging).\n",
    "- By randomly leaving out candidate features from each split, **random forests \"decorrelate\" the trees** to the extent that the averaging process can reduce the variance of the resulting model.\n",
    "- Another way of looking at it is that sometimes one or two strong features dominate every tree in bagging, resulting in essentially the same tree as every predictor. (This is what was meant when saying the trees could be highly correlated.) By using a subset of features to generate each tree, we get a wider variety of predictive trees that do not all use the same dominant features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part-four\"></a>\n",
    "## Part 4: Building and Tuning Decision Trees and Random Forests\n",
    "\n",
    "In this section, we will implement random forests in scikit-learn.\n",
    "\n",
    "- Major League Baseball player data from 1986-87: [data](https://github.com/justmarkham/DAT8/blob/master/data/hitters.csv), [data dictionary](https://cran.r-project.org/web/packages/ISLR/ISLR.pdf) (page 7)\n",
    "- Each observation represents a player.\n",
    "- **Goal:** Predict player salary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in the data.\n",
    "path ='./data/hitters.csv'\n",
    "hitters = pd.read_csv(path)\n",
    "\n",
    "# Remove rows with missing values.\n",
    "hitters.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AtBat</th>\n",
       "      <th>Hits</th>\n",
       "      <th>HmRun</th>\n",
       "      <th>Runs</th>\n",
       "      <th>RBI</th>\n",
       "      <th>Walks</th>\n",
       "      <th>Years</th>\n",
       "      <th>CAtBat</th>\n",
       "      <th>CHits</th>\n",
       "      <th>CHmRun</th>\n",
       "      <th>CRuns</th>\n",
       "      <th>CRBI</th>\n",
       "      <th>CWalks</th>\n",
       "      <th>League</th>\n",
       "      <th>Division</th>\n",
       "      <th>PutOuts</th>\n",
       "      <th>Assists</th>\n",
       "      <th>Errors</th>\n",
       "      <th>Salary</th>\n",
       "      <th>NewLeague</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>315</td>\n",
       "      <td>81</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>14</td>\n",
       "      <td>3449</td>\n",
       "      <td>835</td>\n",
       "      <td>69</td>\n",
       "      <td>321</td>\n",
       "      <td>414</td>\n",
       "      <td>375</td>\n",
       "      <td>N</td>\n",
       "      <td>W</td>\n",
       "      <td>632</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>475.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>479</td>\n",
       "      <td>130</td>\n",
       "      <td>18</td>\n",
       "      <td>66</td>\n",
       "      <td>72</td>\n",
       "      <td>76</td>\n",
       "      <td>3</td>\n",
       "      <td>1624</td>\n",
       "      <td>457</td>\n",
       "      <td>63</td>\n",
       "      <td>224</td>\n",
       "      <td>266</td>\n",
       "      <td>263</td>\n",
       "      <td>A</td>\n",
       "      <td>W</td>\n",
       "      <td>880</td>\n",
       "      <td>82</td>\n",
       "      <td>14</td>\n",
       "      <td>480.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>496</td>\n",
       "      <td>141</td>\n",
       "      <td>20</td>\n",
       "      <td>65</td>\n",
       "      <td>78</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>5628</td>\n",
       "      <td>1575</td>\n",
       "      <td>225</td>\n",
       "      <td>828</td>\n",
       "      <td>838</td>\n",
       "      <td>354</td>\n",
       "      <td>N</td>\n",
       "      <td>E</td>\n",
       "      <td>200</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>500.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>321</td>\n",
       "      <td>87</td>\n",
       "      <td>10</td>\n",
       "      <td>39</td>\n",
       "      <td>42</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>396</td>\n",
       "      <td>101</td>\n",
       "      <td>12</td>\n",
       "      <td>48</td>\n",
       "      <td>46</td>\n",
       "      <td>33</td>\n",
       "      <td>N</td>\n",
       "      <td>E</td>\n",
       "      <td>805</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>91.5</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>594</td>\n",
       "      <td>169</td>\n",
       "      <td>4</td>\n",
       "      <td>74</td>\n",
       "      <td>51</td>\n",
       "      <td>35</td>\n",
       "      <td>11</td>\n",
       "      <td>4408</td>\n",
       "      <td>1133</td>\n",
       "      <td>19</td>\n",
       "      <td>501</td>\n",
       "      <td>336</td>\n",
       "      <td>194</td>\n",
       "      <td>A</td>\n",
       "      <td>W</td>\n",
       "      <td>282</td>\n",
       "      <td>421</td>\n",
       "      <td>25</td>\n",
       "      <td>750.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AtBat  Hits  HmRun  Runs  RBI  Walks  Years  CAtBat  CHits  CHmRun  CRuns  \\\n",
       "1    315    81      7    24   38     39     14    3449    835      69    321   \n",
       "2    479   130     18    66   72     76      3    1624    457      63    224   \n",
       "3    496   141     20    65   78     37     11    5628   1575     225    828   \n",
       "4    321    87     10    39   42     30      2     396    101      12     48   \n",
       "5    594   169      4    74   51     35     11    4408   1133      19    501   \n",
       "\n",
       "   CRBI  CWalks League Division  PutOuts  Assists  Errors  Salary NewLeague  \n",
       "1   414     375      N        W      632       43      10   475.0         N  \n",
       "2   266     263      A        W      880       82      14   480.0         A  \n",
       "3   838     354      N        E      200       11       3   500.0         N  \n",
       "4    46      33      N        E      805       40       4    91.5         N  \n",
       "5   336     194      A        W      282      421      25   750.0         A  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hitters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AtBat</th>\n",
       "      <th>Hits</th>\n",
       "      <th>HmRun</th>\n",
       "      <th>Runs</th>\n",
       "      <th>RBI</th>\n",
       "      <th>Walks</th>\n",
       "      <th>Years</th>\n",
       "      <th>CAtBat</th>\n",
       "      <th>CHits</th>\n",
       "      <th>CHmRun</th>\n",
       "      <th>CRuns</th>\n",
       "      <th>CRBI</th>\n",
       "      <th>CWalks</th>\n",
       "      <th>League</th>\n",
       "      <th>Division</th>\n",
       "      <th>PutOuts</th>\n",
       "      <th>Assists</th>\n",
       "      <th>Errors</th>\n",
       "      <th>Salary</th>\n",
       "      <th>NewLeague</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>315</td>\n",
       "      <td>81</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>14</td>\n",
       "      <td>3449</td>\n",
       "      <td>835</td>\n",
       "      <td>69</td>\n",
       "      <td>321</td>\n",
       "      <td>414</td>\n",
       "      <td>375</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>632</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>475.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>479</td>\n",
       "      <td>130</td>\n",
       "      <td>18</td>\n",
       "      <td>66</td>\n",
       "      <td>72</td>\n",
       "      <td>76</td>\n",
       "      <td>3</td>\n",
       "      <td>1624</td>\n",
       "      <td>457</td>\n",
       "      <td>63</td>\n",
       "      <td>224</td>\n",
       "      <td>266</td>\n",
       "      <td>263</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>880</td>\n",
       "      <td>82</td>\n",
       "      <td>14</td>\n",
       "      <td>480.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>496</td>\n",
       "      <td>141</td>\n",
       "      <td>20</td>\n",
       "      <td>65</td>\n",
       "      <td>78</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>5628</td>\n",
       "      <td>1575</td>\n",
       "      <td>225</td>\n",
       "      <td>828</td>\n",
       "      <td>838</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>321</td>\n",
       "      <td>87</td>\n",
       "      <td>10</td>\n",
       "      <td>39</td>\n",
       "      <td>42</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>396</td>\n",
       "      <td>101</td>\n",
       "      <td>12</td>\n",
       "      <td>48</td>\n",
       "      <td>46</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>805</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>91.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>594</td>\n",
       "      <td>169</td>\n",
       "      <td>4</td>\n",
       "      <td>74</td>\n",
       "      <td>51</td>\n",
       "      <td>35</td>\n",
       "      <td>11</td>\n",
       "      <td>4408</td>\n",
       "      <td>1133</td>\n",
       "      <td>19</td>\n",
       "      <td>501</td>\n",
       "      <td>336</td>\n",
       "      <td>194</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>282</td>\n",
       "      <td>421</td>\n",
       "      <td>25</td>\n",
       "      <td>750.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AtBat  Hits  HmRun  Runs  RBI  Walks  Years  CAtBat  CHits  CHmRun  CRuns  \\\n",
       "1    315    81      7    24   38     39     14    3449    835      69    321   \n",
       "2    479   130     18    66   72     76      3    1624    457      63    224   \n",
       "3    496   141     20    65   78     37     11    5628   1575     225    828   \n",
       "4    321    87     10    39   42     30      2     396    101      12     48   \n",
       "5    594   169      4    74   51     35     11    4408   1133      19    501   \n",
       "\n",
       "   CRBI  CWalks  League  Division  PutOuts  Assists  Errors  Salary  NewLeague  \n",
       "1   414     375       0         0      632       43      10   475.0          0  \n",
       "2   266     263       1         0      880       82      14   480.0          1  \n",
       "3   838     354       0         1      200       11       3   500.0          0  \n",
       "4    46      33       0         1      805       40       4    91.5          0  \n",
       "5   336     194       1         0      282      421      25   750.0          1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode categorical variables as integers.\n",
    "hitters['League'] = pd.factorize(hitters.League)[0]\n",
    "hitters['Division'] = pd.factorize(hitters.Division)[0]\n",
    "hitters['NewLeague'] = pd.factorize(hitters.NewLeague)[0]\n",
    "hitters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Allow plots to appear in the notebook.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAADxCAYAAADY8oDhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f56116b9410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a scatter plot of hits vs years, colored by salary\n",
    "hitters.plot(kind='scatter', x='Years', y='Hits', c='Salary', colormap='jet', xlim=(0, 25), ylim=(0, 250));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'AtBat', u'Hits', u'HmRun', u'Runs', u'RBI', u'Walks', u'Years',\n",
       "       u'League', u'Division', u'PutOuts', u'Assists', u'Errors',\n",
       "       u'NewLeague'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define features: Exclude career statistics (which start with \"C\") and the response (salary).\n",
    "feature_cols = hitters.columns[hitters.columns.str.startswith('C') == False].drop('Salary')\n",
    "feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define X and y.\n",
    "X = hitters[feature_cols]\n",
    "y = hitters.Salary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"decision-tree\"></a>\n",
    "## Optional: Predicting Salary With a Decision Tree\n",
    "\n",
    "Let's first recall how we might predict salary using a single decision tree.\n",
    "\n",
    "We'll first find the best **max_depth** for a decision tree using cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List of values to try for max_depth:\n",
    "max_depth_range = list(range(1, 21))\n",
    "\n",
    "# List to store the average RMSE for each value of max_depth:\n",
    "RMSE_scores = []\n",
    "\n",
    "# Use 10-fold cross-validation with each value of max_depth.\n",
    "from sklearn.model_selection import cross_val_score\n",
    "for depth in max_depth_range:\n",
    "    treereg = DecisionTreeRegressor(max_depth=depth, random_state=1)\n",
    "    MSE_scores = cross_val_score(treereg, X, y, cv=10, scoring='neg_mean_squared_error')\n",
    "    RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAELCAYAAADDZxFQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f56116b9150>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot max_depth (x-axis) versus RMSE (y-axis).\n",
    "plt.plot(max_depth_range, RMSE_scores);\n",
    "plt.xlabel('max_depth');\n",
    "plt.ylabel('RMSE (lower is better)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(340.03416870475201, 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the best RMSE and the corresponding max_depth.\n",
    "sorted(zip(RMSE_scores, max_depth_range))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, presort=False, random_state=1,\n",
       "           splitter='best')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max_depth=2 was best, so fit a tree using that parameter.\n",
    "treereg = DecisionTreeRegressor(max_depth=2, random_state=1)\n",
    "treereg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AtBat</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HmRun</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Runs</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RBI</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Walks</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>League</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Division</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PutOuts</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Assists</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Errors</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NewLeague</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Years</td>\n",
       "      <td>0.488391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hits</td>\n",
       "      <td>0.511609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature  importance\n",
       "0       AtBat    0.000000\n",
       "2       HmRun    0.000000\n",
       "3        Runs    0.000000\n",
       "4         RBI    0.000000\n",
       "5       Walks    0.000000\n",
       "7      League    0.000000\n",
       "8    Division    0.000000\n",
       "9     PutOuts    0.000000\n",
       "10    Assists    0.000000\n",
       "11     Errors    0.000000\n",
       "12  NewLeague    0.000000\n",
       "6       Years    0.488391\n",
       "1        Hits    0.511609"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute feature importances.\n",
    "pd.DataFrame({'feature':feature_cols, 'importance':treereg.feature_importances_}).sort_values(by='importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"random-forest-demo\"></a>\n",
    "## Predicting Salary With a Random Forest\n",
    "\n",
    "### Fitting a Random Forest With the Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features=5, max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=150, n_jobs=1,\n",
       "           oob_score=True, random_state=1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max_features=5 is best and n_estimators=150 is sufficiently large.\n",
    "rfreg = RandomForestRegressor(n_estimators=150, max_features=5, oob_score=True, random_state=1)\n",
    "rfreg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NewLeague</td>\n",
       "      <td>0.004220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>League</td>\n",
       "      <td>0.004647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Division</td>\n",
       "      <td>0.008690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Assists</td>\n",
       "      <td>0.027348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Errors</td>\n",
       "      <td>0.037357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HmRun</td>\n",
       "      <td>0.040278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PutOuts</td>\n",
       "      <td>0.063006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Runs</td>\n",
       "      <td>0.080400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AtBat</td>\n",
       "      <td>0.091355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RBI</td>\n",
       "      <td>0.133843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Walks</td>\n",
       "      <td>0.137514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hits</td>\n",
       "      <td>0.146523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Years</td>\n",
       "      <td>0.224819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature  importance\n",
       "12  NewLeague    0.004220\n",
       "7      League    0.004647\n",
       "8    Division    0.008690\n",
       "10    Assists    0.027348\n",
       "11     Errors    0.037357\n",
       "2       HmRun    0.040278\n",
       "9     PutOuts    0.063006\n",
       "3        Runs    0.080400\n",
       "0       AtBat    0.091355\n",
       "4         RBI    0.133843\n",
       "5       Walks    0.137514\n",
       "1        Hits    0.146523\n",
       "6       Years    0.224819"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute feature importances.\n",
    "pd.DataFrame({'feature':feature_cols, 'importance':rfreg.feature_importances_}).sort_values(by='importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.522926502478\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "293.85082625794416"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the out-of-bag R-squared score.\n",
    "print((rfreg.oob_score_))\n",
    "\n",
    "# Find the average RMSE.\n",
    "scores = cross_val_score(rfreg, X, y, cv=10, scoring='neg_mean_squared_error')\n",
    "np.mean(np.sqrt(-scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducing X to its Most Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(263, 13)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape of X.\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** It important not to select features before separating your train from your test otherwise you are selecting features based on all known observations and introducing more of the information in the test data to the model when you fit it on the training data. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features=5, max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=150, n_jobs=1,\n",
       "           oob_score=True, random_state=1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model on only the train data\n",
    "rfreg = RandomForestRegressor(n_estimators=150, max_features=5, oob_score=True, random_state=1)\n",
    "rfreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(197, 5)\n",
      "(197, 7)\n"
     ]
    }
   ],
   "source": [
    "# Set a threshold for which features to include.\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "print(SelectFromModel(rfreg, threshold='mean', prefit=True).transform(X_train).shape)\n",
    "print(SelectFromModel(rfreg, threshold='median', prefit=True).transform(X_train).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the fit model and the features from the train data to transform the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new feature matrix that only includes important features.\n",
    "\n",
    "X_important =  SelectFromModel(rfreg, threshold='mean', prefit=True).transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314.66052720112219"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the RMSE for a random forest that only includes important features.\n",
    "rfreg = RandomForestRegressor(n_estimators=150, max_features=3, random_state=1)\n",
    "\n",
    "scores = cross_val_score(rfreg, X_important, y_test, cv=10, scoring='neg_mean_squared_error')\n",
    "np.mean(np.sqrt(-scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the error decreased slightly. Often parameter tuning is required to achieve optimal results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"comparing\"></a>\n",
    "## Comparing Random Forests With Decision Trees\n",
    "\n",
    "**Advantages of random forests:**\n",
    "\n",
    "- Their performance is competitive with the best supervised learning methods.\n",
    "- They provide a more reliable estimate of feature importance.\n",
    "- They allow you to estimate out-of-sample error without using train/test split or cross-validation.\n",
    "\n",
    "**Disadvantages of random forests:**\n",
    "\n",
    "- They are less interpretable.\n",
    "- They are slower to train.\n",
    "- They are slower to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Machine learning flowchart](assets/driver_ensembling.png)\n",
    "\n",
    "*Machine learning flowchart created by the [second-place finisher](http://blog.kaggle.com/2015/04/20/axa-winners-interview-learning-telematic-fingerprints-from-gps-data/) of Kaggle's [Driver Telematics competition](https://www.kaggle.com/c/axa-driver-telematics-analysis)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tuning\"></a>\n",
    "## Optional: Tuning Individual Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfreg = RandomForestRegressor()\n",
    "rfreg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning n_estimators\n",
    "\n",
    "One important tuning parameter is **n_estimators**, which represents the number of trees that should be grown. This should be a large enough value that the error seems to have \"stabilized.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List of values to try for n_estimators:\n",
    "estimator_range = list(range(10, 310, 10))\n",
    "\n",
    "# List to store the average RMSE for each value of n_estimators:\n",
    "RMSE_scores = []\n",
    "\n",
    "# Use five-fold cross-validation with each value of n_estimators (Warning: Slow!).\n",
    "for estimator in estimator_range:\n",
    "    rfreg = RandomForestRegressor(n_estimators=estimator, random_state=1)\n",
    "    MSE_scores = cross_val_score(rfreg, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "    RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAELCAYAAADDZxFQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f561c366510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot RMSE (y-axis) versus n_estimators (x-axis).\n",
    "\n",
    "plt.plot(estimator_range, RMSE_scores);\n",
    "\n",
    "plt.xlabel('n_estimators');\n",
    "plt.ylabel('RMSE (lower is better)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_** In theory, the RMSE will continue to decrease and eventually level out.  Adding more estimators will neither (noticably)increase or decrease the RMSE (or other loss metric). However, introduction of noise can lead to random spikes as the n_estimators changes. This example is particularly interesting as after about 120 estimators the RMSE seems to steadily rise as more estimators are added.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning max_features\n",
    "\n",
    "The other important tuning parameter is **max_features**, which represents the number of features that should be considered at each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List of values to try for max_features:\n",
    "feature_range = list(range(1, len(feature_cols)+1))\n",
    "\n",
    "# List to store the average RMSE for each value of max_features:\n",
    "RMSE_scores = []\n",
    "\n",
    "# Use 10-fold cross-validation with each value of max_features (Warning: Super slow!).\n",
    "for feature in feature_range:\n",
    "    rfreg = RandomForestRegressor(n_estimators=150, max_features=feature, random_state=1)\n",
    "    MSE_scores = cross_val_score(rfreg, X, y, cv=10, scoring='neg_mean_squared_error')\n",
    "    RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAELCAYAAADDZxFQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f56116b9ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot max_features (x-axis) versus RMSE (y-axis).\n",
    "\n",
    "plt.plot(feature_range, RMSE_scores);\n",
    "\n",
    "plt.xlabel('max_features');\n",
    "plt.ylabel('RMSE (lower is better)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(290.00785113284348, 10)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the best RMSE and the corresponding max_features.\n",
    "sorted(zip(RMSE_scores, feature_range))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'max_features': feature_range, 'n_estimators': estimator_range}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features=13, max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=150, n_jobs=1,\n",
       "           oob_score=False, random_state=1, verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'max_features': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], 'n_estimators': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, 280, 290, 300]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'max_features': feature_range, 'n_estimators': estimator_range}\n",
    "cv = GridSearchCV(rfreg, parameters)\n",
    "cv.fit(X, y)\n",
    "cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features=12, max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=130, n_jobs=1,\n",
       "           oob_score=False, random_state=1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.021621</td>\n",
       "      <td>0.003740</td>\n",
       "      <td>0.311603</td>\n",
       "      <td>0.877387</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>{u'max_features': 1, u'n_estimators': 10}</td>\n",
       "      <td>389</td>\n",
       "      <td>0.423157</td>\n",
       "      <td>0.876728</td>\n",
       "      <td>0.253576</td>\n",
       "      <td>0.870241</td>\n",
       "      <td>0.257460</td>\n",
       "      <td>0.885191</td>\n",
       "      <td>0.009788</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>0.079121</td>\n",
       "      <td>0.006121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.028374</td>\n",
       "      <td>0.006512</td>\n",
       "      <td>0.372643</td>\n",
       "      <td>0.904294</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>{u'max_features': 1, u'n_estimators': 20}</td>\n",
       "      <td>388</td>\n",
       "      <td>0.446807</td>\n",
       "      <td>0.888446</td>\n",
       "      <td>0.366770</td>\n",
       "      <td>0.908629</td>\n",
       "      <td>0.303567</td>\n",
       "      <td>0.915806</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.058569</td>\n",
       "      <td>0.011583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.042414</td>\n",
       "      <td>0.009663</td>\n",
       "      <td>0.401643</td>\n",
       "      <td>0.911279</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>{u'max_features': 1, u'n_estimators': 30}</td>\n",
       "      <td>382</td>\n",
       "      <td>0.470711</td>\n",
       "      <td>0.897215</td>\n",
       "      <td>0.373437</td>\n",
       "      <td>0.916910</td>\n",
       "      <td>0.360312</td>\n",
       "      <td>0.919710</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.049270</td>\n",
       "      <td>0.010010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.056204</td>\n",
       "      <td>0.012812</td>\n",
       "      <td>0.397514</td>\n",
       "      <td>0.915926</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>{u'max_features': 1, u'n_estimators': 40}</td>\n",
       "      <td>386</td>\n",
       "      <td>0.473660</td>\n",
       "      <td>0.903419</td>\n",
       "      <td>0.359420</td>\n",
       "      <td>0.922275</td>\n",
       "      <td>0.359024</td>\n",
       "      <td>0.922084</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.053998</td>\n",
       "      <td>0.008844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.070460</td>\n",
       "      <td>0.016162</td>\n",
       "      <td>0.391930</td>\n",
       "      <td>0.916628</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>{u'max_features': 1, u'n_estimators': 50}</td>\n",
       "      <td>387</td>\n",
       "      <td>0.465519</td>\n",
       "      <td>0.910617</td>\n",
       "      <td>0.360701</td>\n",
       "      <td>0.922255</td>\n",
       "      <td>0.349082</td>\n",
       "      <td>0.917012</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.052399</td>\n",
       "      <td>0.004759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.085479</td>\n",
       "      <td>0.019247</td>\n",
       "      <td>0.398551</td>\n",
       "      <td>0.914543</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>{u'max_features': 1, u'n_estimators': 60}</td>\n",
       "      <td>384</td>\n",
       "      <td>0.462622</td>\n",
       "      <td>0.906452</td>\n",
       "      <td>0.375061</td>\n",
       "      <td>0.920906</td>\n",
       "      <td>0.357504</td>\n",
       "      <td>0.916270</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.045995</td>\n",
       "      <td>0.006026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.099023</td>\n",
       "      <td>0.022507</td>\n",
       "      <td>0.397845</td>\n",
       "      <td>0.915946</td>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>{u'max_features': 1, u'n_estimators': 70}</td>\n",
       "      <td>385</td>\n",
       "      <td>0.459577</td>\n",
       "      <td>0.904826</td>\n",
       "      <td>0.379517</td>\n",
       "      <td>0.922695</td>\n",
       "      <td>0.353941</td>\n",
       "      <td>0.920317</td>\n",
       "      <td>0.000914</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>0.045002</td>\n",
       "      <td>0.007923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.113207</td>\n",
       "      <td>0.025455</td>\n",
       "      <td>0.400677</td>\n",
       "      <td>0.915498</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>{u'max_features': 1, u'n_estimators': 80}</td>\n",
       "      <td>383</td>\n",
       "      <td>0.453569</td>\n",
       "      <td>0.904922</td>\n",
       "      <td>0.387417</td>\n",
       "      <td>0.922370</td>\n",
       "      <td>0.360590</td>\n",
       "      <td>0.919202</td>\n",
       "      <td>0.000853</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.039070</td>\n",
       "      <td>0.007589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.127576</td>\n",
       "      <td>0.028700</td>\n",
       "      <td>0.404994</td>\n",
       "      <td>0.914642</td>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>{u'max_features': 1, u'n_estimators': 90}</td>\n",
       "      <td>381</td>\n",
       "      <td>0.459215</td>\n",
       "      <td>0.904886</td>\n",
       "      <td>0.384602</td>\n",
       "      <td>0.921291</td>\n",
       "      <td>0.370775</td>\n",
       "      <td>0.917750</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.038861</td>\n",
       "      <td>0.007049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.142491</td>\n",
       "      <td>0.032122</td>\n",
       "      <td>0.410080</td>\n",
       "      <td>0.915883</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>{u'max_features': 1, u'n_estimators': 100}</td>\n",
       "      <td>376</td>\n",
       "      <td>0.466062</td>\n",
       "      <td>0.903581</td>\n",
       "      <td>0.390673</td>\n",
       "      <td>0.924210</td>\n",
       "      <td>0.373085</td>\n",
       "      <td>0.919859</td>\n",
       "      <td>0.000809</td>\n",
       "      <td>0.000367</td>\n",
       "      <td>0.040341</td>\n",
       "      <td>0.008879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.159940</td>\n",
       "      <td>0.035149</td>\n",
       "      <td>0.414642</td>\n",
       "      <td>0.914383</td>\n",
       "      <td>1</td>\n",
       "      <td>110</td>\n",
       "      <td>{u'max_features': 1, u'n_estimators': 110}</td>\n",
       "      <td>363</td>\n",
       "      <td>0.467952</td>\n",
       "      <td>0.903567</td>\n",
       "      <td>0.397564</td>\n",
       "      <td>0.922632</td>\n",
       "      <td>0.377994</td>\n",
       "      <td>0.916951</td>\n",
       "      <td>0.005454</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>0.038636</td>\n",
       "      <td>0.007992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.167783</td>\n",
       "      <td>0.040494</td>\n",
       "      <td>0.415782</td>\n",
       "      <td>0.914823</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>{u'max_features': 1, u'n_estimators': 120}</td>\n",
       "      <td>361</td>\n",
       "      <td>0.471383</td>\n",
       "      <td>0.906278</td>\n",
       "      <td>0.403388</td>\n",
       "      <td>0.922065</td>\n",
       "      <td>0.372079</td>\n",
       "      <td>0.916126</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>0.003406</td>\n",
       "      <td>0.041444</td>\n",
       "      <td>0.006511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.185153</td>\n",
       "      <td>0.042099</td>\n",
       "      <td>0.410231</td>\n",
       "      <td>0.916153</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>{u'max_features': 1, u'n_estimators': 130}</td>\n",
       "      <td>375</td>\n",
       "      <td>0.471152</td>\n",
       "      <td>0.908128</td>\n",
       "      <td>0.400218</td>\n",
       "      <td>0.922853</td>\n",
       "      <td>0.358737</td>\n",
       "      <td>0.917478</td>\n",
       "      <td>0.002197</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>0.046395</td>\n",
       "      <td>0.006084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.208037</td>\n",
       "      <td>0.044379</td>\n",
       "      <td>0.407343</td>\n",
       "      <td>0.915374</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>{u'max_features': 1, u'n_estimators': 140}</td>\n",
       "      <td>379</td>\n",
       "      <td>0.467136</td>\n",
       "      <td>0.907650</td>\n",
       "      <td>0.401151</td>\n",
       "      <td>0.920553</td>\n",
       "      <td>0.353125</td>\n",
       "      <td>0.917918</td>\n",
       "      <td>0.010639</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.046707</td>\n",
       "      <td>0.005566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.211276</td>\n",
       "      <td>0.047037</td>\n",
       "      <td>0.408726</td>\n",
       "      <td>0.915678</td>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>{u'max_features': 1, u'n_estimators': 150}</td>\n",
       "      <td>377</td>\n",
       "      <td>0.468078</td>\n",
       "      <td>0.908034</td>\n",
       "      <td>0.400731</td>\n",
       "      <td>0.920308</td>\n",
       "      <td>0.356778</td>\n",
       "      <td>0.918691</td>\n",
       "      <td>0.003002</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.045747</td>\n",
       "      <td>0.005445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.227952</td>\n",
       "      <td>0.052847</td>\n",
       "      <td>0.407926</td>\n",
       "      <td>0.915707</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>{u'max_features': 1, u'n_estimators': 160}</td>\n",
       "      <td>378</td>\n",
       "      <td>0.467576</td>\n",
       "      <td>0.906275</td>\n",
       "      <td>0.397071</td>\n",
       "      <td>0.920663</td>\n",
       "      <td>0.358572</td>\n",
       "      <td>0.920183</td>\n",
       "      <td>0.004828</td>\n",
       "      <td>0.003829</td>\n",
       "      <td>0.045119</td>\n",
       "      <td>0.006672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.241536</td>\n",
       "      <td>0.054490</td>\n",
       "      <td>0.410764</td>\n",
       "      <td>0.914386</td>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "      <td>{u'max_features': 1, u'n_estimators': 170}</td>\n",
       "      <td>373</td>\n",
       "      <td>0.472353</td>\n",
       "      <td>0.903989</td>\n",
       "      <td>0.404963</td>\n",
       "      <td>0.920797</td>\n",
       "      <td>0.354334</td>\n",
       "      <td>0.918371</td>\n",
       "      <td>0.005351</td>\n",
       "      <td>0.001031</td>\n",
       "      <td>0.048310</td>\n",
       "      <td>0.007418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.255493</td>\n",
       "      <td>0.057366</td>\n",
       "      <td>0.411024</td>\n",
       "      <td>0.915281</td>\n",
       "      <td>1</td>\n",
       "      <td>180</td>\n",
       "      <td>{u'max_features': 1, u'n_estimators': 180}</td>\n",
       "      <td>372</td>\n",
       "      <td>0.473227</td>\n",
       "      <td>0.903452</td>\n",
       "      <td>0.403859</td>\n",
       "      <td>0.922606</td>\n",
       "      <td>0.355352</td>\n",
       "      <td>0.919784</td>\n",
       "      <td>0.002117</td>\n",
       "      <td>0.000683</td>\n",
       "      <td>0.048343</td>\n",
       "      <td>0.008443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.270328</td>\n",
       "      <td>0.069556</td>\n",
       "      <td>0.412206</td>\n",
       "      <td>0.913762</td>\n",
       "      <td>1</td>\n",
       "      <td>190</td>\n",
       "      <td>{u'max_features': 1, u'n_estimators': 190}</td>\n",
       "      <td>366</td>\n",
       "      <td>0.472836</td>\n",
       "      <td>0.903140</td>\n",
       "      <td>0.402496</td>\n",
       "      <td>0.921089</td>\n",
       "      <td>0.360700</td>\n",
       "      <td>0.917055</td>\n",
       "      <td>0.004093</td>\n",
       "      <td>0.011856</td>\n",
       "      <td>0.046250</td>\n",
       "      <td>0.007689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.283638</td>\n",
       "      <td>0.064662</td>\n",
       "      <td>0.416597</td>\n",
       "      <td>0.914298</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>{u'max_features': 1, u'n_estimators': 200}</td>\n",
       "      <td>358</td>\n",
       "      <td>0.477790</td>\n",
       "      <td>0.903720</td>\n",
       "      <td>0.403588</td>\n",
       "      <td>0.921026</td>\n",
       "      <td>0.367858</td>\n",
       "      <td>0.918148</td>\n",
       "      <td>0.002918</td>\n",
       "      <td>0.001545</td>\n",
       "      <td>0.045775</td>\n",
       "      <td>0.007571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.297385</td>\n",
       "      <td>0.066834</td>\n",
       "      <td>0.415153</td>\n",
       "      <td>0.914083</td>\n",
       "      <td>1</td>\n",
       "      <td>210</td>\n",
       "      <td>{u'max_features': 1, u'n_estimators': 210}</td>\n",
       "      <td>362</td>\n",
       "      <td>0.474133</td>\n",
       "      <td>0.902495</td>\n",
       "      <td>0.403271</td>\n",
       "      <td>0.921226</td>\n",
       "      <td>0.367515</td>\n",
       "      <td>0.918528</td>\n",
       "      <td>0.007025</td>\n",
       "      <td>0.001642</td>\n",
       "      <td>0.044294</td>\n",
       "      <td>0.008268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.318706</td>\n",
       "      <td>0.069212</td>\n",
       "      <td>0.415825</td>\n",
       "      <td>0.913672</td>\n",
       "      <td>1</td>\n",
       "      <td>220</td>\n",
       "      <td>{u'max_features': 1, u'n_estimators': 220}</td>\n",
       "      <td>360</td>\n",
       "      <td>0.474298</td>\n",
       "      <td>0.902576</td>\n",
       "      <td>0.402607</td>\n",
       "      <td>0.919994</td>\n",
       "      <td>0.370050</td>\n",
       "      <td>0.918447</td>\n",
       "      <td>0.003729</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.043539</td>\n",
       "      <td>0.007872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.336051</td>\n",
       "      <td>0.073645</td>\n",
       "      <td>0.414132</td>\n",
       "      <td>0.913336</td>\n",
       "      <td>1</td>\n",
       "      <td>230</td>\n",
       "      <td>{u'max_features': 1, u'n_estimators': 230}</td>\n",
       "      <td>364</td>\n",
       "      <td>0.473709</td>\n",
       "      <td>0.902040</td>\n",
       "      <td>0.399770</td>\n",
       "      <td>0.919283</td>\n",
       "      <td>0.368395</td>\n",
       "      <td>0.918686</td>\n",
       "      <td>0.017097</td>\n",
       "      <td>0.001998</td>\n",
       "      <td>0.044143</td>\n",
       "      <td>0.007991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.336395</td>\n",
       "      <td>0.076031</td>\n",
       "      <td>0.413399</td>\n",
       "      <td>0.913157</td>\n",
       "      <td>1</td>\n",
       "      <td>240</td>\n",
       "      <td>{u'max_features': 1, u'n_estimators': 240}</td>\n",
       "      <td>365</td>\n",
       "      <td>0.473201</td>\n",
       "      <td>0.901007</td>\n",
       "      <td>0.401917</td>\n",
       "      <td>0.919594</td>\n",
       "      <td>0.364525</td>\n",
       "      <td>0.918869</td>\n",
       "      <td>0.003405</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>0.045065</td>\n",
       "      <td>0.008597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.358588</td>\n",
       "      <td>0.081220</td>\n",
       "      <td>0.411255</td>\n",
       "      <td>0.912864</td>\n",
       "      <td>1</td>\n",
       "      <td>250</td>\n",
       "      <td>{u'max_features': 1, u'n_estimators': 250}</td>\n",
       "      <td>371</td>\n",
       "      <td>0.473103</td>\n",
       "      <td>0.899311</td>\n",
       "      <td>0.400052</td>\n",
       "      <td>0.920071</td>\n",
       "      <td>0.360028</td>\n",
       "      <td>0.919212</td>\n",
       "      <td>0.004661</td>\n",
       "      <td>0.002039</td>\n",
       "      <td>0.046797</td>\n",
       "      <td>0.009590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.363806</td>\n",
       "      <td>0.081450</td>\n",
       "      <td>0.410756</td>\n",
       "      <td>0.912386</td>\n",
       "      <td>1</td>\n",
       "      <td>260</td>\n",
       "      <td>{u'max_features': 1, u'n_estimators': 260}</td>\n",
       "      <td>374</td>\n",
       "      <td>0.472247</td>\n",
       "      <td>0.898994</td>\n",
       "      <td>0.398143</td>\n",
       "      <td>0.919618</td>\n",
       "      <td>0.361317</td>\n",
       "      <td>0.918545</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.046119</td>\n",
       "      <td>0.009479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.394563</td>\n",
       "      <td>0.085608</td>\n",
       "      <td>0.411801</td>\n",
       "      <td>0.912584</td>\n",
       "      <td>1</td>\n",
       "      <td>270</td>\n",
       "      <td>{u'max_features': 1, u'n_estimators': 270}</td>\n",
       "      <td>369</td>\n",
       "      <td>0.474302</td>\n",
       "      <td>0.899143</td>\n",
       "      <td>0.395734</td>\n",
       "      <td>0.919980</td>\n",
       "      <td>0.364833</td>\n",
       "      <td>0.918629</td>\n",
       "      <td>0.020419</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>0.046078</td>\n",
       "      <td>0.009520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.393243</td>\n",
       "      <td>0.088273</td>\n",
       "      <td>0.412076</td>\n",
       "      <td>0.913000</td>\n",
       "      <td>1</td>\n",
       "      <td>280</td>\n",
       "      <td>{u'max_features': 1, u'n_estimators': 280}</td>\n",
       "      <td>367</td>\n",
       "      <td>0.475588</td>\n",
       "      <td>0.900672</td>\n",
       "      <td>0.396600</td>\n",
       "      <td>0.920293</td>\n",
       "      <td>0.363488</td>\n",
       "      <td>0.918036</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.047019</td>\n",
       "      <td>0.008766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.407556</td>\n",
       "      <td>0.100343</td>\n",
       "      <td>0.411848</td>\n",
       "      <td>0.913760</td>\n",
       "      <td>1</td>\n",
       "      <td>290</td>\n",
       "      <td>{u'max_features': 1, u'n_estimators': 290}</td>\n",
       "      <td>368</td>\n",
       "      <td>0.475727</td>\n",
       "      <td>0.901518</td>\n",
       "      <td>0.395405</td>\n",
       "      <td>0.920870</td>\n",
       "      <td>0.363868</td>\n",
       "      <td>0.918893</td>\n",
       "      <td>0.002070</td>\n",
       "      <td>0.012416</td>\n",
       "      <td>0.047089</td>\n",
       "      <td>0.008694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.419049</td>\n",
       "      <td>0.094283</td>\n",
       "      <td>0.411312</td>\n",
       "      <td>0.913670</td>\n",
       "      <td>1</td>\n",
       "      <td>300</td>\n",
       "      <td>{u'max_features': 1, u'n_estimators': 300}</td>\n",
       "      <td>370</td>\n",
       "      <td>0.475942</td>\n",
       "      <td>0.901231</td>\n",
       "      <td>0.394288</td>\n",
       "      <td>0.921006</td>\n",
       "      <td>0.363157</td>\n",
       "      <td>0.918775</td>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>0.047557</td>\n",
       "      <td>0.008843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>0.018538</td>\n",
       "      <td>0.003591</td>\n",
       "      <td>0.454202</td>\n",
       "      <td>0.897328</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>{u'max_features': 13, u'n_estimators': 10}</td>\n",
       "      <td>353</td>\n",
       "      <td>0.596866</td>\n",
       "      <td>0.890721</td>\n",
       "      <td>0.441943</td>\n",
       "      <td>0.916563</td>\n",
       "      <td>0.322298</td>\n",
       "      <td>0.884700</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.112320</td>\n",
       "      <td>0.013822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>0.037100</td>\n",
       "      <td>0.006945</td>\n",
       "      <td>0.517678</td>\n",
       "      <td>0.918409</td>\n",
       "      <td>13</td>\n",
       "      <td>20</td>\n",
       "      <td>{u'max_features': 13, u'n_estimators': 20}</td>\n",
       "      <td>308</td>\n",
       "      <td>0.622602</td>\n",
       "      <td>0.906822</td>\n",
       "      <td>0.539997</td>\n",
       "      <td>0.930809</td>\n",
       "      <td>0.388973</td>\n",
       "      <td>0.917597</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.096592</td>\n",
       "      <td>0.009809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>0.053763</td>\n",
       "      <td>0.010299</td>\n",
       "      <td>0.554301</td>\n",
       "      <td>0.926266</td>\n",
       "      <td>13</td>\n",
       "      <td>30</td>\n",
       "      <td>{u'max_features': 13, u'n_estimators': 30}</td>\n",
       "      <td>112</td>\n",
       "      <td>0.640088</td>\n",
       "      <td>0.916629</td>\n",
       "      <td>0.559737</td>\n",
       "      <td>0.936486</td>\n",
       "      <td>0.462028</td>\n",
       "      <td>0.925684</td>\n",
       "      <td>0.000597</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.072725</td>\n",
       "      <td>0.008117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>0.072921</td>\n",
       "      <td>0.013719</td>\n",
       "      <td>0.558525</td>\n",
       "      <td>0.928552</td>\n",
       "      <td>13</td>\n",
       "      <td>40</td>\n",
       "      <td>{u'max_features': 13, u'n_estimators': 40}</td>\n",
       "      <td>75</td>\n",
       "      <td>0.641971</td>\n",
       "      <td>0.920321</td>\n",
       "      <td>0.564468</td>\n",
       "      <td>0.936150</td>\n",
       "      <td>0.468107</td>\n",
       "      <td>0.929186</td>\n",
       "      <td>0.001596</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.071036</td>\n",
       "      <td>0.006478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>0.092333</td>\n",
       "      <td>0.016855</td>\n",
       "      <td>0.560346</td>\n",
       "      <td>0.928018</td>\n",
       "      <td>13</td>\n",
       "      <td>50</td>\n",
       "      <td>{u'max_features': 13, u'n_estimators': 50}</td>\n",
       "      <td>54</td>\n",
       "      <td>0.640053</td>\n",
       "      <td>0.922268</td>\n",
       "      <td>0.560350</td>\n",
       "      <td>0.936400</td>\n",
       "      <td>0.479719</td>\n",
       "      <td>0.925387</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.065393</td>\n",
       "      <td>0.006062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>0.113680</td>\n",
       "      <td>0.020775</td>\n",
       "      <td>0.560690</td>\n",
       "      <td>0.927020</td>\n",
       "      <td>13</td>\n",
       "      <td>60</td>\n",
       "      <td>{u'max_features': 13, u'n_estimators': 60}</td>\n",
       "      <td>49</td>\n",
       "      <td>0.639395</td>\n",
       "      <td>0.922272</td>\n",
       "      <td>0.559329</td>\n",
       "      <td>0.933222</td>\n",
       "      <td>0.482456</td>\n",
       "      <td>0.925565</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>0.001252</td>\n",
       "      <td>0.064015</td>\n",
       "      <td>0.004587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>0.130206</td>\n",
       "      <td>0.023892</td>\n",
       "      <td>0.560815</td>\n",
       "      <td>0.926627</td>\n",
       "      <td>13</td>\n",
       "      <td>70</td>\n",
       "      <td>{u'max_features': 13, u'n_estimators': 70}</td>\n",
       "      <td>44</td>\n",
       "      <td>0.652450</td>\n",
       "      <td>0.919121</td>\n",
       "      <td>0.554873</td>\n",
       "      <td>0.934494</td>\n",
       "      <td>0.474138</td>\n",
       "      <td>0.926267</td>\n",
       "      <td>0.001593</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.072847</td>\n",
       "      <td>0.006281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>0.143732</td>\n",
       "      <td>0.027021</td>\n",
       "      <td>0.562007</td>\n",
       "      <td>0.927794</td>\n",
       "      <td>13</td>\n",
       "      <td>80</td>\n",
       "      <td>{u'max_features': 13, u'n_estimators': 80}</td>\n",
       "      <td>31</td>\n",
       "      <td>0.652883</td>\n",
       "      <td>0.922270</td>\n",
       "      <td>0.556479</td>\n",
       "      <td>0.936166</td>\n",
       "      <td>0.475679</td>\n",
       "      <td>0.924946</td>\n",
       "      <td>0.002365</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>0.072379</td>\n",
       "      <td>0.006020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>0.161302</td>\n",
       "      <td>0.029948</td>\n",
       "      <td>0.561954</td>\n",
       "      <td>0.928252</td>\n",
       "      <td>13</td>\n",
       "      <td>90</td>\n",
       "      <td>{u'max_features': 13, u'n_estimators': 90}</td>\n",
       "      <td>32</td>\n",
       "      <td>0.651041</td>\n",
       "      <td>0.924435</td>\n",
       "      <td>0.557070</td>\n",
       "      <td>0.936082</td>\n",
       "      <td>0.476783</td>\n",
       "      <td>0.924239</td>\n",
       "      <td>0.002497</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.071156</td>\n",
       "      <td>0.005537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>0.182340</td>\n",
       "      <td>0.034874</td>\n",
       "      <td>0.563642</td>\n",
       "      <td>0.927624</td>\n",
       "      <td>13</td>\n",
       "      <td>100</td>\n",
       "      <td>{u'max_features': 13, u'n_estimators': 100}</td>\n",
       "      <td>24</td>\n",
       "      <td>0.648218</td>\n",
       "      <td>0.921161</td>\n",
       "      <td>0.561282</td>\n",
       "      <td>0.935625</td>\n",
       "      <td>0.480482</td>\n",
       "      <td>0.926087</td>\n",
       "      <td>0.002510</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>0.068432</td>\n",
       "      <td>0.006004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>0.200018</td>\n",
       "      <td>0.037242</td>\n",
       "      <td>0.564796</td>\n",
       "      <td>0.926651</td>\n",
       "      <td>13</td>\n",
       "      <td>110</td>\n",
       "      <td>{u'max_features': 13, u'n_estimators': 110}</td>\n",
       "      <td>14</td>\n",
       "      <td>0.645364</td>\n",
       "      <td>0.921666</td>\n",
       "      <td>0.561567</td>\n",
       "      <td>0.934904</td>\n",
       "      <td>0.486567</td>\n",
       "      <td>0.923384</td>\n",
       "      <td>0.002681</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.064806</td>\n",
       "      <td>0.005878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>0.219967</td>\n",
       "      <td>0.050738</td>\n",
       "      <td>0.563649</td>\n",
       "      <td>0.927426</td>\n",
       "      <td>13</td>\n",
       "      <td>120</td>\n",
       "      <td>{u'max_features': 13, u'n_estimators': 120}</td>\n",
       "      <td>23</td>\n",
       "      <td>0.642639</td>\n",
       "      <td>0.924703</td>\n",
       "      <td>0.563656</td>\n",
       "      <td>0.934879</td>\n",
       "      <td>0.483745</td>\n",
       "      <td>0.922695</td>\n",
       "      <td>0.002101</td>\n",
       "      <td>0.013568</td>\n",
       "      <td>0.064805</td>\n",
       "      <td>0.005333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>0.233728</td>\n",
       "      <td>0.042453</td>\n",
       "      <td>0.565163</td>\n",
       "      <td>0.927716</td>\n",
       "      <td>13</td>\n",
       "      <td>130</td>\n",
       "      <td>{u'max_features': 13, u'n_estimators': 130}</td>\n",
       "      <td>12</td>\n",
       "      <td>0.642023</td>\n",
       "      <td>0.925559</td>\n",
       "      <td>0.567302</td>\n",
       "      <td>0.934066</td>\n",
       "      <td>0.485256</td>\n",
       "      <td>0.923524</td>\n",
       "      <td>0.001509</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.063956</td>\n",
       "      <td>0.004566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>0.251275</td>\n",
       "      <td>0.047671</td>\n",
       "      <td>0.563788</td>\n",
       "      <td>0.927039</td>\n",
       "      <td>13</td>\n",
       "      <td>140</td>\n",
       "      <td>{u'max_features': 13, u'n_estimators': 140}</td>\n",
       "      <td>22</td>\n",
       "      <td>0.640401</td>\n",
       "      <td>0.925069</td>\n",
       "      <td>0.565258</td>\n",
       "      <td>0.931662</td>\n",
       "      <td>0.484809</td>\n",
       "      <td>0.924386</td>\n",
       "      <td>0.000820</td>\n",
       "      <td>0.001068</td>\n",
       "      <td>0.063467</td>\n",
       "      <td>0.003281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>0.278723</td>\n",
       "      <td>0.051840</td>\n",
       "      <td>0.559486</td>\n",
       "      <td>0.927980</td>\n",
       "      <td>13</td>\n",
       "      <td>150</td>\n",
       "      <td>{u'max_features': 13, u'n_estimators': 150}</td>\n",
       "      <td>66</td>\n",
       "      <td>0.639874</td>\n",
       "      <td>0.926137</td>\n",
       "      <td>0.557326</td>\n",
       "      <td>0.932558</td>\n",
       "      <td>0.480358</td>\n",
       "      <td>0.925246</td>\n",
       "      <td>0.001763</td>\n",
       "      <td>0.001162</td>\n",
       "      <td>0.065077</td>\n",
       "      <td>0.003257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>0.303801</td>\n",
       "      <td>0.057057</td>\n",
       "      <td>0.558972</td>\n",
       "      <td>0.927820</td>\n",
       "      <td>13</td>\n",
       "      <td>160</td>\n",
       "      <td>{u'max_features': 13, u'n_estimators': 160}</td>\n",
       "      <td>71</td>\n",
       "      <td>0.639580</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.558755</td>\n",
       "      <td>0.931792</td>\n",
       "      <td>0.477657</td>\n",
       "      <td>0.926669</td>\n",
       "      <td>0.003613</td>\n",
       "      <td>0.005220</td>\n",
       "      <td>0.066041</td>\n",
       "      <td>0.002890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>0.328531</td>\n",
       "      <td>0.059821</td>\n",
       "      <td>0.560860</td>\n",
       "      <td>0.927454</td>\n",
       "      <td>13</td>\n",
       "      <td>170</td>\n",
       "      <td>{u'max_features': 13, u'n_estimators': 170}</td>\n",
       "      <td>43</td>\n",
       "      <td>0.640474</td>\n",
       "      <td>0.923765</td>\n",
       "      <td>0.560934</td>\n",
       "      <td>0.931864</td>\n",
       "      <td>0.480257</td>\n",
       "      <td>0.926732</td>\n",
       "      <td>0.006465</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>0.065345</td>\n",
       "      <td>0.003346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>0.335548</td>\n",
       "      <td>0.062657</td>\n",
       "      <td>0.560312</td>\n",
       "      <td>0.928348</td>\n",
       "      <td>13</td>\n",
       "      <td>180</td>\n",
       "      <td>{u'max_features': 13, u'n_estimators': 180}</td>\n",
       "      <td>55</td>\n",
       "      <td>0.640541</td>\n",
       "      <td>0.923105</td>\n",
       "      <td>0.559487</td>\n",
       "      <td>0.933870</td>\n",
       "      <td>0.479994</td>\n",
       "      <td>0.928070</td>\n",
       "      <td>0.001398</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.065482</td>\n",
       "      <td>0.004399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>0.369110</td>\n",
       "      <td>0.065717</td>\n",
       "      <td>0.560755</td>\n",
       "      <td>0.927275</td>\n",
       "      <td>13</td>\n",
       "      <td>190</td>\n",
       "      <td>{u'max_features': 13, u'n_estimators': 190}</td>\n",
       "      <td>47</td>\n",
       "      <td>0.642182</td>\n",
       "      <td>0.922406</td>\n",
       "      <td>0.557951</td>\n",
       "      <td>0.933136</td>\n",
       "      <td>0.481228</td>\n",
       "      <td>0.926283</td>\n",
       "      <td>0.015099</td>\n",
       "      <td>0.001033</td>\n",
       "      <td>0.065676</td>\n",
       "      <td>0.004437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>0.385568</td>\n",
       "      <td>0.068975</td>\n",
       "      <td>0.561001</td>\n",
       "      <td>0.927390</td>\n",
       "      <td>13</td>\n",
       "      <td>200</td>\n",
       "      <td>{u'max_features': 13, u'n_estimators': 200}</td>\n",
       "      <td>42</td>\n",
       "      <td>0.642286</td>\n",
       "      <td>0.922459</td>\n",
       "      <td>0.558515</td>\n",
       "      <td>0.933164</td>\n",
       "      <td>0.481296</td>\n",
       "      <td>0.926548</td>\n",
       "      <td>0.001966</td>\n",
       "      <td>0.001436</td>\n",
       "      <td>0.065684</td>\n",
       "      <td>0.004411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>0.401331</td>\n",
       "      <td>0.072720</td>\n",
       "      <td>0.560755</td>\n",
       "      <td>0.927344</td>\n",
       "      <td>13</td>\n",
       "      <td>210</td>\n",
       "      <td>{u'max_features': 13, u'n_estimators': 210}</td>\n",
       "      <td>46</td>\n",
       "      <td>0.642486</td>\n",
       "      <td>0.921579</td>\n",
       "      <td>0.558439</td>\n",
       "      <td>0.933114</td>\n",
       "      <td>0.480428</td>\n",
       "      <td>0.927340</td>\n",
       "      <td>0.004419</td>\n",
       "      <td>0.001101</td>\n",
       "      <td>0.066116</td>\n",
       "      <td>0.004709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>0.414354</td>\n",
       "      <td>0.075286</td>\n",
       "      <td>0.559946</td>\n",
       "      <td>0.926450</td>\n",
       "      <td>13</td>\n",
       "      <td>220</td>\n",
       "      <td>{u'max_features': 13, u'n_estimators': 220}</td>\n",
       "      <td>62</td>\n",
       "      <td>0.642216</td>\n",
       "      <td>0.921011</td>\n",
       "      <td>0.555361</td>\n",
       "      <td>0.931232</td>\n",
       "      <td>0.481368</td>\n",
       "      <td>0.927108</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.065683</td>\n",
       "      <td>0.004199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>0.447772</td>\n",
       "      <td>0.079889</td>\n",
       "      <td>0.559141</td>\n",
       "      <td>0.926482</td>\n",
       "      <td>13</td>\n",
       "      <td>230</td>\n",
       "      <td>{u'max_features': 13, u'n_estimators': 230}</td>\n",
       "      <td>68</td>\n",
       "      <td>0.642208</td>\n",
       "      <td>0.921244</td>\n",
       "      <td>0.556518</td>\n",
       "      <td>0.931003</td>\n",
       "      <td>0.477771</td>\n",
       "      <td>0.927199</td>\n",
       "      <td>0.013791</td>\n",
       "      <td>0.001648</td>\n",
       "      <td>0.067092</td>\n",
       "      <td>0.004016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>0.448592</td>\n",
       "      <td>0.080541</td>\n",
       "      <td>0.557617</td>\n",
       "      <td>0.925740</td>\n",
       "      <td>13</td>\n",
       "      <td>240</td>\n",
       "      <td>{u'max_features': 13, u'n_estimators': 240}</td>\n",
       "      <td>87</td>\n",
       "      <td>0.640657</td>\n",
       "      <td>0.919653</td>\n",
       "      <td>0.555123</td>\n",
       "      <td>0.930894</td>\n",
       "      <td>0.476146</td>\n",
       "      <td>0.926673</td>\n",
       "      <td>0.004447</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>0.067120</td>\n",
       "      <td>0.004636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>0.471009</td>\n",
       "      <td>0.086467</td>\n",
       "      <td>0.556899</td>\n",
       "      <td>0.925275</td>\n",
       "      <td>13</td>\n",
       "      <td>250</td>\n",
       "      <td>{u'max_features': 13, u'n_estimators': 250}</td>\n",
       "      <td>95</td>\n",
       "      <td>0.641941</td>\n",
       "      <td>0.918165</td>\n",
       "      <td>0.555504</td>\n",
       "      <td>0.931106</td>\n",
       "      <td>0.472291</td>\n",
       "      <td>0.926552</td>\n",
       "      <td>0.010121</td>\n",
       "      <td>0.000884</td>\n",
       "      <td>0.069200</td>\n",
       "      <td>0.005360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>0.485350</td>\n",
       "      <td>0.088831</td>\n",
       "      <td>0.556922</td>\n",
       "      <td>0.924890</td>\n",
       "      <td>13</td>\n",
       "      <td>260</td>\n",
       "      <td>{u'max_features': 13, u'n_estimators': 260}</td>\n",
       "      <td>94</td>\n",
       "      <td>0.641887</td>\n",
       "      <td>0.918158</td>\n",
       "      <td>0.554739</td>\n",
       "      <td>0.930852</td>\n",
       "      <td>0.473190</td>\n",
       "      <td>0.925659</td>\n",
       "      <td>0.007770</td>\n",
       "      <td>0.001115</td>\n",
       "      <td>0.068821</td>\n",
       "      <td>0.005211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>0.507065</td>\n",
       "      <td>0.092821</td>\n",
       "      <td>0.557764</td>\n",
       "      <td>0.925278</td>\n",
       "      <td>13</td>\n",
       "      <td>270</td>\n",
       "      <td>{u'max_features': 13, u'n_estimators': 270}</td>\n",
       "      <td>85</td>\n",
       "      <td>0.642736</td>\n",
       "      <td>0.918536</td>\n",
       "      <td>0.555125</td>\n",
       "      <td>0.931535</td>\n",
       "      <td>0.474486</td>\n",
       "      <td>0.925763</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.001549</td>\n",
       "      <td>0.068647</td>\n",
       "      <td>0.005318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>0.527853</td>\n",
       "      <td>0.094133</td>\n",
       "      <td>0.557854</td>\n",
       "      <td>0.925750</td>\n",
       "      <td>13</td>\n",
       "      <td>280</td>\n",
       "      <td>{u'max_features': 13, u'n_estimators': 280}</td>\n",
       "      <td>82</td>\n",
       "      <td>0.641733</td>\n",
       "      <td>0.919957</td>\n",
       "      <td>0.554125</td>\n",
       "      <td>0.931269</td>\n",
       "      <td>0.476782</td>\n",
       "      <td>0.926026</td>\n",
       "      <td>0.004563</td>\n",
       "      <td>0.000865</td>\n",
       "      <td>0.067328</td>\n",
       "      <td>0.004622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>0.541591</td>\n",
       "      <td>0.099912</td>\n",
       "      <td>0.559140</td>\n",
       "      <td>0.926325</td>\n",
       "      <td>13</td>\n",
       "      <td>290</td>\n",
       "      <td>{u'max_features': 13, u'n_estimators': 290}</td>\n",
       "      <td>69</td>\n",
       "      <td>0.644347</td>\n",
       "      <td>0.920805</td>\n",
       "      <td>0.553527</td>\n",
       "      <td>0.931670</td>\n",
       "      <td>0.478632</td>\n",
       "      <td>0.926501</td>\n",
       "      <td>0.006333</td>\n",
       "      <td>0.001835</td>\n",
       "      <td>0.067704</td>\n",
       "      <td>0.004437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>0.592524</td>\n",
       "      <td>0.105155</td>\n",
       "      <td>0.558678</td>\n",
       "      <td>0.926349</td>\n",
       "      <td>13</td>\n",
       "      <td>300</td>\n",
       "      <td>{u'max_features': 13, u'n_estimators': 300}</td>\n",
       "      <td>73</td>\n",
       "      <td>0.642511</td>\n",
       "      <td>0.920571</td>\n",
       "      <td>0.554568</td>\n",
       "      <td>0.931939</td>\n",
       "      <td>0.478037</td>\n",
       "      <td>0.926537</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.002101</td>\n",
       "      <td>0.067144</td>\n",
       "      <td>0.004643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>390 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0         0.021621         0.003740         0.311603          0.877387   \n",
       "1         0.028374         0.006512         0.372643          0.904294   \n",
       "2         0.042414         0.009663         0.401643          0.911279   \n",
       "3         0.056204         0.012812         0.397514          0.915926   \n",
       "4         0.070460         0.016162         0.391930          0.916628   \n",
       "5         0.085479         0.019247         0.398551          0.914543   \n",
       "6         0.099023         0.022507         0.397845          0.915946   \n",
       "7         0.113207         0.025455         0.400677          0.915498   \n",
       "8         0.127576         0.028700         0.404994          0.914642   \n",
       "9         0.142491         0.032122         0.410080          0.915883   \n",
       "10        0.159940         0.035149         0.414642          0.914383   \n",
       "11        0.167783         0.040494         0.415782          0.914823   \n",
       "12        0.185153         0.042099         0.410231          0.916153   \n",
       "13        0.208037         0.044379         0.407343          0.915374   \n",
       "14        0.211276         0.047037         0.408726          0.915678   \n",
       "15        0.227952         0.052847         0.407926          0.915707   \n",
       "16        0.241536         0.054490         0.410764          0.914386   \n",
       "17        0.255493         0.057366         0.411024          0.915281   \n",
       "18        0.270328         0.069556         0.412206          0.913762   \n",
       "19        0.283638         0.064662         0.416597          0.914298   \n",
       "20        0.297385         0.066834         0.415153          0.914083   \n",
       "21        0.318706         0.069212         0.415825          0.913672   \n",
       "22        0.336051         0.073645         0.414132          0.913336   \n",
       "23        0.336395         0.076031         0.413399          0.913157   \n",
       "24        0.358588         0.081220         0.411255          0.912864   \n",
       "25        0.363806         0.081450         0.410756          0.912386   \n",
       "26        0.394563         0.085608         0.411801          0.912584   \n",
       "27        0.393243         0.088273         0.412076          0.913000   \n",
       "28        0.407556         0.100343         0.411848          0.913760   \n",
       "29        0.419049         0.094283         0.411312          0.913670   \n",
       "..             ...              ...              ...               ...   \n",
       "360       0.018538         0.003591         0.454202          0.897328   \n",
       "361       0.037100         0.006945         0.517678          0.918409   \n",
       "362       0.053763         0.010299         0.554301          0.926266   \n",
       "363       0.072921         0.013719         0.558525          0.928552   \n",
       "364       0.092333         0.016855         0.560346          0.928018   \n",
       "365       0.113680         0.020775         0.560690          0.927020   \n",
       "366       0.130206         0.023892         0.560815          0.926627   \n",
       "367       0.143732         0.027021         0.562007          0.927794   \n",
       "368       0.161302         0.029948         0.561954          0.928252   \n",
       "369       0.182340         0.034874         0.563642          0.927624   \n",
       "370       0.200018         0.037242         0.564796          0.926651   \n",
       "371       0.219967         0.050738         0.563649          0.927426   \n",
       "372       0.233728         0.042453         0.565163          0.927716   \n",
       "373       0.251275         0.047671         0.563788          0.927039   \n",
       "374       0.278723         0.051840         0.559486          0.927980   \n",
       "375       0.303801         0.057057         0.558972          0.927820   \n",
       "376       0.328531         0.059821         0.560860          0.927454   \n",
       "377       0.335548         0.062657         0.560312          0.928348   \n",
       "378       0.369110         0.065717         0.560755          0.927275   \n",
       "379       0.385568         0.068975         0.561001          0.927390   \n",
       "380       0.401331         0.072720         0.560755          0.927344   \n",
       "381       0.414354         0.075286         0.559946          0.926450   \n",
       "382       0.447772         0.079889         0.559141          0.926482   \n",
       "383       0.448592         0.080541         0.557617          0.925740   \n",
       "384       0.471009         0.086467         0.556899          0.925275   \n",
       "385       0.485350         0.088831         0.556922          0.924890   \n",
       "386       0.507065         0.092821         0.557764          0.925278   \n",
       "387       0.527853         0.094133         0.557854          0.925750   \n",
       "388       0.541591         0.099912         0.559140          0.926325   \n",
       "389       0.592524         0.105155         0.558678          0.926349   \n",
       "\n",
       "    param_max_features param_n_estimators  \\\n",
       "0                    1                 10   \n",
       "1                    1                 20   \n",
       "2                    1                 30   \n",
       "3                    1                 40   \n",
       "4                    1                 50   \n",
       "5                    1                 60   \n",
       "6                    1                 70   \n",
       "7                    1                 80   \n",
       "8                    1                 90   \n",
       "9                    1                100   \n",
       "10                   1                110   \n",
       "11                   1                120   \n",
       "12                   1                130   \n",
       "13                   1                140   \n",
       "14                   1                150   \n",
       "15                   1                160   \n",
       "16                   1                170   \n",
       "17                   1                180   \n",
       "18                   1                190   \n",
       "19                   1                200   \n",
       "20                   1                210   \n",
       "21                   1                220   \n",
       "22                   1                230   \n",
       "23                   1                240   \n",
       "24                   1                250   \n",
       "25                   1                260   \n",
       "26                   1                270   \n",
       "27                   1                280   \n",
       "28                   1                290   \n",
       "29                   1                300   \n",
       "..                 ...                ...   \n",
       "360                 13                 10   \n",
       "361                 13                 20   \n",
       "362                 13                 30   \n",
       "363                 13                 40   \n",
       "364                 13                 50   \n",
       "365                 13                 60   \n",
       "366                 13                 70   \n",
       "367                 13                 80   \n",
       "368                 13                 90   \n",
       "369                 13                100   \n",
       "370                 13                110   \n",
       "371                 13                120   \n",
       "372                 13                130   \n",
       "373                 13                140   \n",
       "374                 13                150   \n",
       "375                 13                160   \n",
       "376                 13                170   \n",
       "377                 13                180   \n",
       "378                 13                190   \n",
       "379                 13                200   \n",
       "380                 13                210   \n",
       "381                 13                220   \n",
       "382                 13                230   \n",
       "383                 13                240   \n",
       "384                 13                250   \n",
       "385                 13                260   \n",
       "386                 13                270   \n",
       "387                 13                280   \n",
       "388                 13                290   \n",
       "389                 13                300   \n",
       "\n",
       "                                          params  rank_test_score  \\\n",
       "0      {u'max_features': 1, u'n_estimators': 10}              389   \n",
       "1      {u'max_features': 1, u'n_estimators': 20}              388   \n",
       "2      {u'max_features': 1, u'n_estimators': 30}              382   \n",
       "3      {u'max_features': 1, u'n_estimators': 40}              386   \n",
       "4      {u'max_features': 1, u'n_estimators': 50}              387   \n",
       "5      {u'max_features': 1, u'n_estimators': 60}              384   \n",
       "6      {u'max_features': 1, u'n_estimators': 70}              385   \n",
       "7      {u'max_features': 1, u'n_estimators': 80}              383   \n",
       "8      {u'max_features': 1, u'n_estimators': 90}              381   \n",
       "9     {u'max_features': 1, u'n_estimators': 100}              376   \n",
       "10    {u'max_features': 1, u'n_estimators': 110}              363   \n",
       "11    {u'max_features': 1, u'n_estimators': 120}              361   \n",
       "12    {u'max_features': 1, u'n_estimators': 130}              375   \n",
       "13    {u'max_features': 1, u'n_estimators': 140}              379   \n",
       "14    {u'max_features': 1, u'n_estimators': 150}              377   \n",
       "15    {u'max_features': 1, u'n_estimators': 160}              378   \n",
       "16    {u'max_features': 1, u'n_estimators': 170}              373   \n",
       "17    {u'max_features': 1, u'n_estimators': 180}              372   \n",
       "18    {u'max_features': 1, u'n_estimators': 190}              366   \n",
       "19    {u'max_features': 1, u'n_estimators': 200}              358   \n",
       "20    {u'max_features': 1, u'n_estimators': 210}              362   \n",
       "21    {u'max_features': 1, u'n_estimators': 220}              360   \n",
       "22    {u'max_features': 1, u'n_estimators': 230}              364   \n",
       "23    {u'max_features': 1, u'n_estimators': 240}              365   \n",
       "24    {u'max_features': 1, u'n_estimators': 250}              371   \n",
       "25    {u'max_features': 1, u'n_estimators': 260}              374   \n",
       "26    {u'max_features': 1, u'n_estimators': 270}              369   \n",
       "27    {u'max_features': 1, u'n_estimators': 280}              367   \n",
       "28    {u'max_features': 1, u'n_estimators': 290}              368   \n",
       "29    {u'max_features': 1, u'n_estimators': 300}              370   \n",
       "..                                           ...              ...   \n",
       "360   {u'max_features': 13, u'n_estimators': 10}              353   \n",
       "361   {u'max_features': 13, u'n_estimators': 20}              308   \n",
       "362   {u'max_features': 13, u'n_estimators': 30}              112   \n",
       "363   {u'max_features': 13, u'n_estimators': 40}               75   \n",
       "364   {u'max_features': 13, u'n_estimators': 50}               54   \n",
       "365   {u'max_features': 13, u'n_estimators': 60}               49   \n",
       "366   {u'max_features': 13, u'n_estimators': 70}               44   \n",
       "367   {u'max_features': 13, u'n_estimators': 80}               31   \n",
       "368   {u'max_features': 13, u'n_estimators': 90}               32   \n",
       "369  {u'max_features': 13, u'n_estimators': 100}               24   \n",
       "370  {u'max_features': 13, u'n_estimators': 110}               14   \n",
       "371  {u'max_features': 13, u'n_estimators': 120}               23   \n",
       "372  {u'max_features': 13, u'n_estimators': 130}               12   \n",
       "373  {u'max_features': 13, u'n_estimators': 140}               22   \n",
       "374  {u'max_features': 13, u'n_estimators': 150}               66   \n",
       "375  {u'max_features': 13, u'n_estimators': 160}               71   \n",
       "376  {u'max_features': 13, u'n_estimators': 170}               43   \n",
       "377  {u'max_features': 13, u'n_estimators': 180}               55   \n",
       "378  {u'max_features': 13, u'n_estimators': 190}               47   \n",
       "379  {u'max_features': 13, u'n_estimators': 200}               42   \n",
       "380  {u'max_features': 13, u'n_estimators': 210}               46   \n",
       "381  {u'max_features': 13, u'n_estimators': 220}               62   \n",
       "382  {u'max_features': 13, u'n_estimators': 230}               68   \n",
       "383  {u'max_features': 13, u'n_estimators': 240}               87   \n",
       "384  {u'max_features': 13, u'n_estimators': 250}               95   \n",
       "385  {u'max_features': 13, u'n_estimators': 260}               94   \n",
       "386  {u'max_features': 13, u'n_estimators': 270}               85   \n",
       "387  {u'max_features': 13, u'n_estimators': 280}               82   \n",
       "388  {u'max_features': 13, u'n_estimators': 290}               69   \n",
       "389  {u'max_features': 13, u'n_estimators': 300}               73   \n",
       "\n",
       "     split0_test_score  split0_train_score  split1_test_score  \\\n",
       "0             0.423157            0.876728           0.253576   \n",
       "1             0.446807            0.888446           0.366770   \n",
       "2             0.470711            0.897215           0.373437   \n",
       "3             0.473660            0.903419           0.359420   \n",
       "4             0.465519            0.910617           0.360701   \n",
       "5             0.462622            0.906452           0.375061   \n",
       "6             0.459577            0.904826           0.379517   \n",
       "7             0.453569            0.904922           0.387417   \n",
       "8             0.459215            0.904886           0.384602   \n",
       "9             0.466062            0.903581           0.390673   \n",
       "10            0.467952            0.903567           0.397564   \n",
       "11            0.471383            0.906278           0.403388   \n",
       "12            0.471152            0.908128           0.400218   \n",
       "13            0.467136            0.907650           0.401151   \n",
       "14            0.468078            0.908034           0.400731   \n",
       "15            0.467576            0.906275           0.397071   \n",
       "16            0.472353            0.903989           0.404963   \n",
       "17            0.473227            0.903452           0.403859   \n",
       "18            0.472836            0.903140           0.402496   \n",
       "19            0.477790            0.903720           0.403588   \n",
       "20            0.474133            0.902495           0.403271   \n",
       "21            0.474298            0.902576           0.402607   \n",
       "22            0.473709            0.902040           0.399770   \n",
       "23            0.473201            0.901007           0.401917   \n",
       "24            0.473103            0.899311           0.400052   \n",
       "25            0.472247            0.898994           0.398143   \n",
       "26            0.474302            0.899143           0.395734   \n",
       "27            0.475588            0.900672           0.396600   \n",
       "28            0.475727            0.901518           0.395405   \n",
       "29            0.475942            0.901231           0.394288   \n",
       "..                 ...                 ...                ...   \n",
       "360           0.596866            0.890721           0.441943   \n",
       "361           0.622602            0.906822           0.539997   \n",
       "362           0.640088            0.916629           0.559737   \n",
       "363           0.641971            0.920321           0.564468   \n",
       "364           0.640053            0.922268           0.560350   \n",
       "365           0.639395            0.922272           0.559329   \n",
       "366           0.652450            0.919121           0.554873   \n",
       "367           0.652883            0.922270           0.556479   \n",
       "368           0.651041            0.924435           0.557070   \n",
       "369           0.648218            0.921161           0.561282   \n",
       "370           0.645364            0.921666           0.561567   \n",
       "371           0.642639            0.924703           0.563656   \n",
       "372           0.642023            0.925559           0.567302   \n",
       "373           0.640401            0.925069           0.565258   \n",
       "374           0.639874            0.926137           0.557326   \n",
       "375           0.639580            0.925000           0.558755   \n",
       "376           0.640474            0.923765           0.560934   \n",
       "377           0.640541            0.923105           0.559487   \n",
       "378           0.642182            0.922406           0.557951   \n",
       "379           0.642286            0.922459           0.558515   \n",
       "380           0.642486            0.921579           0.558439   \n",
       "381           0.642216            0.921011           0.555361   \n",
       "382           0.642208            0.921244           0.556518   \n",
       "383           0.640657            0.919653           0.555123   \n",
       "384           0.641941            0.918165           0.555504   \n",
       "385           0.641887            0.918158           0.554739   \n",
       "386           0.642736            0.918536           0.555125   \n",
       "387           0.641733            0.919957           0.554125   \n",
       "388           0.644347            0.920805           0.553527   \n",
       "389           0.642511            0.920571           0.554568   \n",
       "\n",
       "     split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
       "0              0.870241           0.257460            0.885191      0.009788   \n",
       "1              0.908629           0.303567            0.915806      0.000078   \n",
       "2              0.916910           0.360312            0.919710      0.000184   \n",
       "3              0.922275           0.359024            0.922084      0.000067   \n",
       "4              0.922255           0.349082            0.917012      0.000405   \n",
       "5              0.920906           0.357504            0.916270      0.000844   \n",
       "6              0.922695           0.353941            0.920317      0.000914   \n",
       "7              0.922370           0.360590            0.919202      0.000853   \n",
       "8              0.921291           0.370775            0.917750      0.001072   \n",
       "9              0.924210           0.373085            0.919859      0.000809   \n",
       "10             0.922632           0.377994            0.916951      0.005454   \n",
       "11             0.922065           0.372079            0.916126      0.000385   \n",
       "12             0.922853           0.358737            0.917478      0.002197   \n",
       "13             0.920553           0.353125            0.917918      0.010639   \n",
       "14             0.920308           0.356778            0.918691      0.003002   \n",
       "15             0.920663           0.358572            0.920183      0.004828   \n",
       "16             0.920797           0.354334            0.918371      0.005351   \n",
       "17             0.922606           0.355352            0.919784      0.002117   \n",
       "18             0.921089           0.360700            0.917055      0.004093   \n",
       "19             0.921026           0.367858            0.918148      0.002918   \n",
       "20             0.921226           0.367515            0.918528      0.007025   \n",
       "21             0.919994           0.370050            0.918447      0.003729   \n",
       "22             0.919283           0.368395            0.918686      0.017097   \n",
       "23             0.919594           0.364525            0.918869      0.003405   \n",
       "24             0.920071           0.360028            0.919212      0.004661   \n",
       "25             0.919618           0.361317            0.918545      0.000814   \n",
       "26             0.919980           0.364833            0.918629      0.020419   \n",
       "27             0.920293           0.363488            0.918036      0.000707   \n",
       "28             0.920870           0.363868            0.918893      0.002070   \n",
       "29             0.921006           0.363157            0.918775      0.000593   \n",
       "..                  ...                ...                 ...           ...   \n",
       "360            0.916563           0.322298            0.884700      0.000518   \n",
       "361            0.930809           0.388973            0.917597      0.000747   \n",
       "362            0.936486           0.462028            0.925684      0.000597   \n",
       "363            0.936150           0.468107            0.929186      0.001596   \n",
       "364            0.936400           0.479719            0.925387      0.000588   \n",
       "365            0.933222           0.482456            0.925565      0.001695   \n",
       "366            0.934494           0.474138            0.926267      0.001593   \n",
       "367            0.936166           0.475679            0.924946      0.002365   \n",
       "368            0.936082           0.476783            0.924239      0.002497   \n",
       "369            0.935625           0.480482            0.926087      0.002510   \n",
       "370            0.934904           0.486567            0.923384      0.002681   \n",
       "371            0.934879           0.483745            0.922695      0.002101   \n",
       "372            0.934066           0.485256            0.923524      0.001509   \n",
       "373            0.931662           0.484809            0.924386      0.000820   \n",
       "374            0.932558           0.480358            0.925246      0.001763   \n",
       "375            0.931792           0.477657            0.926669      0.003613   \n",
       "376            0.931864           0.480257            0.926732      0.006465   \n",
       "377            0.933870           0.479994            0.928070      0.001398   \n",
       "378            0.933136           0.481228            0.926283      0.015099   \n",
       "379            0.933164           0.481296            0.926548      0.001966   \n",
       "380            0.933114           0.480428            0.927340      0.004419   \n",
       "381            0.931232           0.481368            0.927108      0.000757   \n",
       "382            0.931003           0.477771            0.927199      0.013791   \n",
       "383            0.930894           0.476146            0.926673      0.004447   \n",
       "384            0.931106           0.472291            0.926552      0.010121   \n",
       "385            0.930852           0.473190            0.925659      0.007770   \n",
       "386            0.931535           0.474486            0.925763      0.002953   \n",
       "387            0.931269           0.476782            0.926026      0.004563   \n",
       "388            0.931670           0.478632            0.926501      0.006333   \n",
       "389            0.931939           0.478037            0.926537      0.022472   \n",
       "\n",
       "     std_score_time  std_test_score  std_train_score  \n",
       "0          0.000408        0.079121         0.006121  \n",
       "1          0.000070        0.058569         0.011583  \n",
       "2          0.000026        0.049270         0.010010  \n",
       "3          0.000033        0.053998         0.008844  \n",
       "4          0.000337        0.052399         0.004759  \n",
       "5          0.000137        0.045995         0.006026  \n",
       "6          0.000274        0.045002         0.007923  \n",
       "7          0.000173        0.039070         0.007589  \n",
       "8          0.000319        0.038861         0.007049  \n",
       "9          0.000367        0.040341         0.008879  \n",
       "10         0.000501        0.038636         0.007992  \n",
       "11         0.003406        0.041444         0.006511  \n",
       "12         0.000844        0.046395         0.006084  \n",
       "13         0.000450        0.046707         0.005566  \n",
       "14         0.000043        0.045747         0.005445  \n",
       "15         0.003829        0.045119         0.006672  \n",
       "16         0.001031        0.048310         0.007418  \n",
       "17         0.000683        0.048343         0.008443  \n",
       "18         0.011856        0.046250         0.007689  \n",
       "19         0.001545        0.045775         0.007571  \n",
       "20         0.001642        0.044294         0.008268  \n",
       "21         0.000040        0.043539         0.007872  \n",
       "22         0.001998        0.044143         0.007991  \n",
       "23         0.001126        0.045065         0.008597  \n",
       "24         0.002039        0.046797         0.009590  \n",
       "25         0.000147        0.046119         0.009479  \n",
       "26         0.000936        0.046078         0.009520  \n",
       "27         0.000512        0.047019         0.008766  \n",
       "28         0.012416        0.047089         0.008694  \n",
       "29         0.000518        0.047557         0.008843  \n",
       "..              ...             ...              ...  \n",
       "360        0.000056        0.112320         0.013822  \n",
       "361        0.000124        0.096592         0.009809  \n",
       "362        0.000089        0.072725         0.008117  \n",
       "363        0.000262        0.071036         0.006478  \n",
       "364        0.000185        0.065393         0.006062  \n",
       "365        0.001252        0.064015         0.004587  \n",
       "366        0.000118        0.072847         0.006281  \n",
       "367        0.000355        0.072379         0.006020  \n",
       "368        0.000442        0.071156         0.005537  \n",
       "369        0.000958        0.068432         0.006004  \n",
       "370        0.000663        0.064806         0.005878  \n",
       "371        0.013568        0.064805         0.005333  \n",
       "372        0.000125        0.063956         0.004566  \n",
       "373        0.001068        0.063467         0.003281  \n",
       "374        0.001162        0.065077         0.003257  \n",
       "375        0.005220        0.066041         0.002890  \n",
       "376        0.000342        0.065345         0.003346  \n",
       "377        0.000391        0.065482         0.004399  \n",
       "378        0.001033        0.065676         0.004437  \n",
       "379        0.001436        0.065684         0.004411  \n",
       "380        0.001101        0.066116         0.004709  \n",
       "381        0.000604        0.065683         0.004199  \n",
       "382        0.001648        0.067092         0.004016  \n",
       "383        0.000713        0.067120         0.004636  \n",
       "384        0.000884        0.069200         0.005360  \n",
       "385        0.001115        0.068821         0.005211  \n",
       "386        0.001549        0.068647         0.005318  \n",
       "387        0.000865        0.067328         0.004622  \n",
       "388        0.001835        0.067704         0.004437  \n",
       "389        0.002101        0.067144         0.004643  \n",
       "\n",
       "[390 rows x 18 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cv.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"summary\"></a>\n",
    "## Summary\n",
    "\n",
    "**Which model is best?** The best classifier for a particular task is task-dependent. In many business cases, interpretability is more important than accuracy. So, decision trees may be preferred. In other cases, accuracy on unseen data might be paramount, in which case random forests would likely be better (since they typically overfit less). \n",
    "\n",
    "Remember that every model is a tradeoff between bias and variance. Ensemble models attempt to reduce overfitting by reducing variance but increasing bias (as compared to decision trees). By making the model more stable, we necessarily make it fit the training data less accurately. In some cases this is desired (particularly if we start with lots of overfitting), but for more simply structured data a simple decision tree might be best.\n",
    "\n",
    "---\n",
    "\n",
    "**In this lesson:**\n",
    "\n",
    "- We looked at ensemble models.\n",
    "\n",
    "- We saw how decision trees could be extended using two ensemble techniques -- bagging and random forests.\n",
    "\n",
    "- We looked at methods of evaluating feature importance and tuning parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
